{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import pandas as pd \n",
    "import scipy.sparse as sp\n",
    "from scipy.sparse import vstack\n",
    "from scipy import sparse\n",
    "from scipy.sparse.linalg import spsolve\n",
    "from subprocess import check_output\n",
    "from sklearn.model_selection import train_test_split\n",
    "from numpy import linalg as LA\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv('/Users/lmac/Documents/inf_search/movie-recomendation-fall-2020/train.txt', header = None, sep = \"\\t\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.columns = [\"UserId\", \"FilmId\", \"Mark\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_users = train['UserId'].nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_items = train['FilmId'].max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train['Mark'].nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train['Mark'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "R = np.zeros((n_users, n_items))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for row in train.itertuples():\n",
    "    R[(row[1]-1), (row[2]-1)] = row[3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "R"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "R.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r_lambda = 0.1\n",
    "nf = 50\n",
    "alpha = 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nu = R.shape[0]\n",
    "ni = R.shape[1]\n",
    "\n",
    "X = np.random.rand(nu, nf) * 0.01\n",
    "Y = np.random.rand(ni, nf) * 0.01\n",
    "\n",
    "print(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "P = np.copy(R)\n",
    "#P[P > 0] = 1\n",
    "print(P)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "C = 1 + alpha * R\n",
    "print(C)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss_function(C, P, xTy, X, Y, r_lambda):\n",
    "    predict_error = np.square(P - xTy)\n",
    "    confidence_error = np.sum(C * predict_error)\n",
    "    regularization = r_lambda * (np.sum(np.square(X)) + np.sum(np.square(Y)))\n",
    "    total_loss = confidence_error + regularization\n",
    "    return np.sum(predict_error), confidence_error, regularization, total_loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def optimize_user(X, Y, C, P, nu, nf, r_lambda):\n",
    "    yT = np.transpose(Y)\n",
    "    for u in range(nu):\n",
    "        Cu = np.diag(C[u])\n",
    "        yT_Cu_y = np.matmul(np.matmul(yT, Cu), Y)\n",
    "        lI = np.dot(r_lambda, np.identity(nf))\n",
    "        yT_Cu_pu = np.matmul(np.matmul(yT, Cu), P[u])\n",
    "        X[u] = np.linalg.solve(yT_Cu_y + lI, yT_Cu_pu)\n",
    "\n",
    "def optimize_item(X, Y, C, P, ni, nf, r_lambda):\n",
    "    xT = np.transpose(X)\n",
    "    for i in range(ni):\n",
    "        Ci = np.diag(C[:, i])\n",
    "        xT_Ci_x = np.matmul(np.matmul(xT, Ci), X)\n",
    "        lI = np.dot(r_lambda, np.identity(nf))\n",
    "        xT_Ci_pi = np.matmul(np.matmul(xT, Ci), P[:, i])\n",
    "        Y[i] = np.linalg.solve(xT_Ci_x + lI, xT_Ci_pi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predict_errors = []\n",
    "confidence_errors = []\n",
    "regularization_list = []\n",
    "total_losses = []\n",
    "\n",
    "for i in range(15):\n",
    "    if i!=0:   \n",
    "        optimize_user(X, Y, C, P, nu, nf, r_lambda)\n",
    "        optimize_item(X, Y, C, P, ni, nf, r_lambda)\n",
    "    predict = np.matmul(X, np.transpose(Y))\n",
    "    predict_error, confidence_error, regularization, total_loss = loss_function(C, P, predict, X, Y, r_lambda)\n",
    "    \n",
    "    predict_errors.append(predict_error)\n",
    "    confidence_errors.append(confidence_error)\n",
    "    regularization_list.append(regularization)\n",
    "    total_losses.append(total_loss)\n",
    "    \n",
    "    print('----------------step %d----------------' % i)\n",
    "    print(\"predict error: %f\" % predict_error)\n",
    "    print(\"confidence error: %f\" % confidence_error)\n",
    "    print(\"regularization: %f\" % regularization)\n",
    "    print(\"total loss: %f\" % total_loss)\n",
    "    \n",
    "predict = np.matmul(X, np.transpose(Y))\n",
    "print('final predict')\n",
    "print([predict])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predict.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.sqrt(np.mean((R-predict)**2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(10):\n",
    "    if i!=0:   \n",
    "        optimize_user(X, Y, C, P, nu, nf, r_lambda)\n",
    "        optimize_item(X, Y, C, P, ni, nf, r_lambda)\n",
    "    predict = np.matmul(X, np.transpose(Y))\n",
    "    predict_error, confidence_error, regularization, total_loss = loss_function(C, P, predict, X, Y, r_lambda)\n",
    "    \n",
    "    predict_errors.append(predict_error)\n",
    "    confidence_errors.append(confidence_error)\n",
    "    regularization_list.append(regularization)\n",
    "    total_losses.append(total_loss)\n",
    "    \n",
    "    print('----------------step %d----------------' % i)\n",
    "    print(\"predict error: %f\" % predict_error)\n",
    "    print(\"confidence error: %f\" % confidence_error)\n",
    "    print(\"regularization: %f\" % regularization)\n",
    "    print(\"total loss: %f\" % total_loss)\n",
    "    \n",
    "predict = np.matmul(X, np.transpose(Y))\n",
    "print('final predict')\n",
    "print([predict])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from numpy import linalg as LA\n",
    "LA.norm(R-predict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(R[R != 0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predict[R != 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LA.norm(R[R != 0] - predict[R != 0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.max(R[R != 0] - predict[R != 0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predict[0, 32]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predict[0, 19]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.min(predict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predict[predict < 1] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.min(predict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"submission_wals.csv\", 'w') as write_file:\n",
    "    print(\"Id,Score\", file=write_file)\n",
    "    for test_id in range(len(test)):\n",
    "            n_row = test[\"UserId\"][test_id] - 1\n",
    "            n_col = test[\"FilmId\"][test_id] - 1\n",
    "            print(f\"{test_id + 1},{predict[n_row, n_col]}\", file=write_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def implicit_als(sparse_data, alpha_val=40, iterations=10, lambda_val=0.1, features=10):\n",
    "\n",
    "\n",
    "    # Calculate the confidence for each value in our data\n",
    "    confidence = sparse_data * alpha_val\n",
    "    \n",
    "    # Get the size of user rows and item columns\n",
    "    user_size, item_size = sparse_data.shape\n",
    "    \n",
    "    # We create the user vectors X of size users-by-features, the item vectors\n",
    "    # Y of size items-by-features and randomly assign the values.\n",
    "    X = sparse.csr_matrix(np.random.normal(size = (user_size, features)))\n",
    "    Y = sparse.csr_matrix(np.random.normal(size = (item_size, features)))\n",
    "    \n",
    "    #Precompute I and lambda * I\n",
    "    X_I = sparse.eye(user_size)\n",
    "    Y_I = sparse.eye(item_size)\n",
    "    \n",
    "    I = sparse.eye(features)\n",
    "    lI = lambda_val * I\n",
    "    \n",
    "    # Start main loop. For each iteration we first compute X and then Y\n",
    "    for i in range(iterations):\n",
    "        print('iteration %d of %d' % (i+1, iterations))\n",
    "        \n",
    "        # Precompute Y-transpose-Y and X-transpose-X\n",
    "        yTy = Y.T.dot(Y)\n",
    "        xTx = X.T.dot(X)\n",
    "\n",
    "        # Loop through all users\n",
    "        for u in range(user_size):\n",
    "\n",
    "            # Get the user row.\n",
    "            u_row = confidence[u,:].toarray() \n",
    "\n",
    "            # Calculate the binary preference p(u)\n",
    "            #p_u = u_row.copy()\n",
    "            #p_u[p_u != 0] = 1.0\n",
    "            p_u = u_row.copy()\n",
    "            p_u = p_u/alpha_val\n",
    "            \n",
    "            # Calculate Cu and Cu - I\n",
    "            CuI = sparse.diags(u_row, [0])\n",
    "            Cu = CuI + Y_I\n",
    "\n",
    "            # Put it all together and compute the final formula\n",
    "            yT_CuI_y = Y.T.dot(CuI).dot(Y)\n",
    "            yT_Cu_pu = Y.T.dot(Cu).dot(p_u.T)\n",
    "            X[u] = spsolve(yTy + yT_CuI_y + lI, yT_Cu_pu)\n",
    "\n",
    "    \n",
    "        for i in range(item_size):\n",
    "\n",
    "            # Get the item column and transpose it.\n",
    "            i_row = confidence[:,i].T.toarray()\n",
    "\n",
    "            # Calculate the binary preference p(i)\n",
    "            #p_i = i_row.copy()\n",
    "            #p_i[p_i != 0] = 1.0\n",
    "            p_i = i_row.copy()\n",
    "            p_i = p_i/alpha_val\n",
    "            \n",
    "            # Calculate Ci and Ci - I\n",
    "            CiI = sparse.diags(i_row, [0])\n",
    "            Ci = CiI + X_I\n",
    "\n",
    "            # Put it all together and compute the final formula\n",
    "            xT_CiI_x = X.T.dot(CiI).dot(X)\n",
    "            xT_Ci_pi = X.T.dot(Ci).dot(p_i.T)\n",
    "            Y[i] = spsolve(xTx + xT_CiI_x + lI, xT_Ci_pi)\n",
    "\n",
    "    return X, Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_sparse = sparse.csr_matrix(R)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_vecs, item_vecs = implicit_als(data_sparse, iterations=10, features=6, alpha_val=40)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_vecs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predict = user_vecs.dot(item_vecs.T).toarray()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LA.norm(R[R != 0] - predict[R != 0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def nonzeros(m, row):\n",
    "    for index in range(m.indptr[row], m.indptr[row+1]):\n",
    "        yield m.indices[index], m.data[index]\n",
    "      \n",
    "      \n",
    "def implicit_als_cg(Cui, features=100, iterations=50, lambda_val=0.01):\n",
    "    user_size, item_size = Cui.shape\n",
    "\n",
    "    X = np.random.rand(user_size, features) * 0.01\n",
    "    Y = np.random.rand(item_size, features) * 0.01\n",
    "\n",
    "    Cui, Ciu = Cui.tocsr(), Cui.T.tocsr()\n",
    "\n",
    "    for iteration in range(iterations):\n",
    "        #if (iteration + 1) % 100 == 0:\n",
    "        print('iteration %d of %d' % (iteration+1, iterations))\n",
    "        least_squares_cg(Cui, X, Y, lambda_val)\n",
    "        least_squares_cg(Ciu, Y, X, lambda_val)\n",
    "    \n",
    "    return sparse.csr_matrix(X), sparse.csr_matrix(Y)\n",
    "  \n",
    "\n",
    "def least_squares_cg(Cui, X, Y, lambda_val, cg_steps=3):\n",
    "    users, features = X.shape\n",
    "    \n",
    "    YtY = Y.T.dot(Y) + lambda_val * np.eye(features)\n",
    "\n",
    "    for u in range(users):\n",
    "\n",
    "        x = X[u]\n",
    "        r = -YtY.dot(x)\n",
    "\n",
    "        for i, confidence in nonzeros(Cui, u):\n",
    "            r += (confidence - (confidence - 1) * Y[i].dot(x)) * Y[i]\n",
    "\n",
    "        p = r.copy()\n",
    "        rsold = r.dot(r)\n",
    "\n",
    "        for it in range(cg_steps):\n",
    "            Ap = YtY.dot(p)\n",
    "            for i, confidence in nonzeros(Cui, u):\n",
    "                Ap += (confidence - 1) * Y[i].dot(p) * Y[i]\n",
    "\n",
    "            alpha = rsold / p.dot(Ap)\n",
    "            x += alpha * p\n",
    "            r -= alpha * Ap\n",
    "\n",
    "            rsnew = r.dot(r)\n",
    "            p = r + (rsnew / rsold) * p\n",
    "            rsold = rsnew\n",
    "\n",
    "        X[u] = x\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alpha_val_list = [100]\n",
    "features_list = [250, 300]\n",
    "lambda_val_list = [0.01, 0.1]\n",
    "a = 1\n",
    "\n",
    "for alpha_val in alpha_val_list:\n",
    "    for features1 in features_list:\n",
    "            for lambda_val1 in lambda_val_list:\n",
    "                conf_data = (data_sparse * alpha_val).astype('double')\n",
    "                user_vecs, item_vecs = implicit_als_cg(conf_data, features1, 200, lambda_val1)\n",
    "                item_vecs = item_vecs.toarray()\n",
    "                # Calculate the vector norms\n",
    "                item_norms = np.sqrt((item_vecs * item_vecs).sum(axis=1))\n",
    "\n",
    "                final_score = []\n",
    "\n",
    "                with open(\"submission_als\" + str(a) + \".csv\", 'w') as write_file:\n",
    "                    print(\"Id,Score\", file=write_file)\n",
    "                    for test_id in range(len(test)):\n",
    "                        user_id = test[\"UserId\"][test_id]\n",
    "                        find_film = test[\"FilmId\"][test_id]            \n",
    "                        scores = item_vecs.dot(item_vecs[find_film - 1].T) / item_norms\n",
    "                        scores = scores * mask[user_id - 1, :]\n",
    "                        num_view = np.sum(scores > 0)\n",
    "                        if num_view == 0:\n",
    "                            cur_score = 5\n",
    "                        else:\n",
    "                            max_sim = np.argsort(-scores)[:min(num_view, 10)]\n",
    "                            cur_score = np.mean(R[user_id - 1, max_sim])\n",
    "                    final_score.append(cur_score)        \n",
    "                    print(f\"{test_id + 1},{cur_score}\", file=write_file)\n",
    "                    \n",
    "                predictions = np.array(final_score)\n",
    "                rmse = np.sqrt(np.mean((predictions-targets)**2))\n",
    "                a = a + 1    \n",
    "                print(alpha_val, features1, lambda_val1, rmse)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "def runALS(A, R):\n",
    "    '''\n",
    "    Runs Alternating Least Squares algorithm in order to calculate matrix.\n",
    "    :param A: User-Item Matrix with ratings\n",
    "    :param R: User-Item Matrix with 1 if there is a rating or 0 if not\n",
    "    :param n_factors: How many factors each of user and item matrix will consider\n",
    "    :param n_iterations: How many times to run algorithm\n",
    "    :param lambda_: Regularization parameter\n",
    "    :return:\n",
    "    '''\n",
    "    print(\"Initiating \")\n",
    "    lambda_ = 0.1; n_factors = 40; n, m = A.shape; n_iterations = 20\n",
    "    Users = 5 * np.random.rand(n, n_factors)\n",
    "    Items = 5 * np.random.rand(n_factors, m)\n",
    "\n",
    "    def get_error(A, Users, Items, R):\n",
    "        # This calculates the MSE of nonzero elements\n",
    "        return np.sum((R * (A - np.dot(Users, Items))) ** 2) / np.sum(R)\n",
    "\n",
    "    MSE_List = []\n",
    "\n",
    "    print(\"Starting Iterations\")\n",
    "    for iter in range(n_iterations):\n",
    "        for i, Ri in enumerate(R):\n",
    "            Users[i] = np.linalg.solve(np.dot(Items, np.dot(np.diag(Ri), Items.T)) + lambda_ * np.eye(n_factors),\n",
    "                                       np.dot(Items, np.dot(np.diag(Ri), A[i].T))).T\n",
    "        print(\"Error after solving for User Matrix:\", get_error(A, Users, Items, R))\n",
    "\n",
    "        for j, Rj in enumerate(R.T):\n",
    "            Items[:,j] = np.linalg.solve(np.dot(Users.T, np.dot(np.diag(Rj), Users)) + lambda_ * np.eye(n_factors),\n",
    "                                     np.dot(Users.T, np.dot(np.diag(Rj), A[:, j])))\n",
    "        print(\"Error after solving for Item Matrix:\", get_error(A, Users, Items, R))\n",
    "\n",
    "        MSE_List.append(get_error(A, Users, Items, R))\n",
    "        print('%sth iteration is complete...' % iter)\n",
    "\n",
    "    print(MSE_List)\n",
    "    fig = plt.figure()\n",
    "    ax = fig.add_subplot(111)\n",
    "    plt.plot(range(1, len(MSE_List) + 1), MSE_List); plt.ylabel('Error'); plt.xlabel('Iteration')\n",
    "    plt.title('Python Implementation MSE by Iteration \\n with %d users and %d movies' % A.shape);\n",
    "    plt.savefig('Python MSE Graph.pdf', format='pdf')\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PP = np.copy(R)\n",
    "PP[PP > 0] = 1\n",
    "\n",
    "runALS(R, PP)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alg_res = pd.read_csv('/Users/lmac/Documents/inf_search/submission_als1.csv')\n",
    "\n",
    "predictions = alg_res['Score'].to_numpy()\n",
    "rmse = np.sqrt(np.mean((predictions-targets)**2))\n",
    "rmse            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alpha_val = 100\n",
    "conf_data = (data_sparse * alpha_val).astype('double')\n",
    "user_vecs, item_vecs = implicit_als_cg(conf_data, iterations=100, features=250, lambda_val=0.5)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predict = user_vecs.dot(item_vecs.T).toarray()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predict = user_vecs.dot(item_vecs.T)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.min(predict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LA.norm(mask[mask != 0] - predict[R != 0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def recommend(user_id, data_sparse, user_vecs, item_vecs, num_items=10):\n",
    "\n",
    "    # This is where we calculate the recommendation by taking the \n",
    "    # dot-product of the user vectors with the item vectors.\n",
    "    rec_vector = user_vecs[user_id,:].dot(item_vecs.T).toarray()\n",
    "    \n",
    "    return rec_vector\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = recommend(1, data_sparse, user_vecs, item_vecs, num_items=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predict = user_vecs.dot(item_vecs.T).toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.max(predict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.min(predict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = pd.read_csv('/Users/lmac/Documents/inf_search/movie-recomendation-fall-2020/test.txt', header = None, sep = \"\\t\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test.columns = [\"UserId\", \"FilmId\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"submission_als.csv\", 'w') as write_file:\n",
    "    print(\"Id,Score\", file=write_file)\n",
    "    for test_id in range(len(test)):\n",
    "            n_row = test[\"UserId\"][test_id] - 1\n",
    "            n_col = test[\"FilmId\"][test_id] - 1\n",
    "            print(f\"{test_id + 1},{predict[n_row, n_col]}\", file=write_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test.iloc[[test_id]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import scipy.sparse as sparse\n",
    "from scipy.sparse.linalg import spsolve\n",
    "import random\n",
    "\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "import implicit\n",
    "\n",
    "\n",
    "# The implicit library expects data as a item-user matrix so we\n",
    "# create two matricies, one for fitting the model (item-user) \n",
    "# and one for recommendations (user-item)\n",
    "sparse_item_user = sparse.csr_matrix((train['Mark'].astype(float), (train['FilmId'], train['UserId'])))\n",
    "sparse_user_item = sparse.csr_matrix((train['Mark'].astype(float), (train['UserId'], train['FilmId'])))\n",
    "\n",
    "# Initialize the als model and fit it using the sparse item-user matrix\n",
    "model = implicit.als.AlternatingLeastSquares(factors=150, regularization=0.05, iterations=50, calculate_training_loss=True)\n",
    "\n",
    "# Calculate the confidence by multiplying it by our alpha value.\n",
    "alpha_val = 100\n",
    "data_conf = (sparse_item_user * alpha_val).astype('double')\n",
    "\n",
    "#Fit the model\n",
    "model.fit(data_conf)\n",
    "\n",
    "\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Create recommendations for user with id 1\n",
    "user_id = 1\n",
    "\n",
    "# Use the implicit recommender.\n",
    "recommended = model.recommend(user_id, sparse_user_item, N = n_items)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(recommended, columns =['Id', 'Score'])\n",
    "df[df['Id'] == 20]['Score']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "min(df['Score'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max(df['Score'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "scaler = MinMaxScaler(feature_range=(1, 5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_id = 1\n",
    "recommended = model.recommend(user_id, sparse_user_item, N = n_items)\n",
    "find_film = 20\n",
    "df = pd.DataFrame(recommended, columns =['Id', 'Score'])\n",
    "df[['Score']] = scaler.fit_transform(df[['Score']])\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "recommended"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cur_score = df.loc[df['Id'] == 1, 'Score'].iloc[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cur_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "similar = model.similar_items(1, n_items)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(similar, columns =['Id', 'Score'])\n",
    "\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "min(df['Score'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max(df['Score'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "\n",
    "with open(\"submission_als5.csv\", 'w') as write_file:\n",
    "    print(\"Id,Score\", file=write_file)\n",
    "    for test_id in range(len(test)):\n",
    "            user_id = test[\"UserId\"][test_id]\n",
    "            recommended = model.recommend(user_id, sparse_user_item, N = n_items)\n",
    "            find_film = test[\"FilmId\"][test_id]\n",
    "            df = pd.DataFrame(recommended, columns =['Id', 'Score'])\n",
    "            df[['Score']] = scaler.fit_transform(df[['Score']])\n",
    "            cur_score = df.loc[df['Id'] == find_film, 'Score'].iloc[0]\n",
    "            print(f\"{test_id + 1},{cur_score}\", file=write_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mask = np.copy(R)\n",
    "mask[mask > 0] = 1\n",
    "\n",
    "mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "\n",
    "with open(\"submission_als11.csv\", 'w') as write_file:\n",
    "    print(\"Id,Score\", file=write_file)\n",
    "    for test_id in range(len(test)):\n",
    "            user_id = test[\"UserId\"][test_id]\n",
    "            find_film = test[\"FilmId\"][test_id]            \n",
    "            similar = model.similar_items(find_film, n_items)\n",
    "            df = pd.DataFrame(similar, columns =['Id', 'Score'])\n",
    "            df[['Score']] = scaler.fit_transform(df[['Score']])\n",
    "            cur_scores = df['Score'].values * mask[user_id - 1, :]\n",
    "            #weights = R[user_id - 1, :]\n",
    "            num_view = np.sum(cur_scores > 0)\n",
    "            if num_view == 0:\n",
    "                 cur_score = 5\n",
    "            else:\n",
    "                max_sim = np.argsort(-cur_scores)[:min(num_view, 10)]\n",
    "                #cur_score = np.dot(cur_score,weights)\n",
    "                cur_score = np.mean(R[user_id - 1, max_sim])\n",
    "            print(f\"{test_id + 1},{cur_score}\", file=write_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "item_vecs = item_vecs.toarray()\n",
    "user_vecs = user_vecs.toarray()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Calculate the vector norms\n",
    "item_norms = np.sqrt((item_vecs * item_vecs).sum(axis=1))\n",
    "final_score = []\n",
    "\n",
    "with open(\"submission_als1.csv\", 'w') as write_file:\n",
    "    print(\"Id,Score\", file=write_file)\n",
    "    for test_id in range(len(test)):\n",
    "            user_id = test[\"UserId\"][test_id]\n",
    "            find_film = test[\"FilmId\"][test_id]            \n",
    "            scores = item_vecs.dot(item_vecs[find_film - 1].T) / item_norms\n",
    "            scores = scores * mask[user_id - 1, :]\n",
    "            num_view = np.sum(scores > 0)\n",
    "            if num_view == 0:\n",
    "                 cur_score = 5\n",
    "            else:\n",
    "                max_sim = np.argsort(-scores)[:min(num_view, 10)]\n",
    "                cur_score = np.mean(R[user_id - 1, max_sim])\n",
    "            final_score.append(cur_score)     \n",
    "            print(f\"{test_id + 1},{cur_score}\", file=write_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cosine_similarity_items = cosine_similarity(item_vecs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cosine_similarity_items.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cosine_similarity_items[0, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "scaler = MinMaxScaler(feature_range=(1, 5))\n",
    "recommend_score = []\n",
    "recommend_score_orig = []\n",
    "\n",
    "\n",
    "with open(\"submission_als2.csv\", 'w') as write_file:\n",
    "    print(\"Id,Score\", file=write_file)\n",
    "    for test_id in range(len(test)):\n",
    "            user_id = test[\"UserId\"][test_id] - 1\n",
    "            recommended = user_vecs[user_id,:].dot(item_vecs.T)\n",
    "            find_film = test[\"FilmId\"][test_id] - 1\n",
    "            recommend_score_orig.append(recommended[find_film])\n",
    "            recommended = scaler.fit_transform(recommended.reshape(-1, 1))\n",
    "            cur_score = recommended[find_film].item(0)\n",
    "            recommend_score.append(cur_score)\n",
    "            print(f\"{test_id + 1},{cur_score}\", file=write_file)\n",
    "            \n",
    "recommend_score = np.array(recommend_score)\n",
    "recommend_score_orig = np.array(recommend_score_orig)\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "recommend_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "scaler = MinMaxScaler(feature_range=(1, 5))\n",
    "predict = user_vecs.dot(item_vecs.T)\n",
    "predict_orig = predict.copy()\n",
    "predict = scaler.fit_transform(predict)\n",
    "recommend_score = []\n",
    "recommend_score_orig = []\n",
    "\n",
    "\n",
    "with open(\"submission_als2.csv\", 'w') as write_file:\n",
    "    print(\"Id,Score\", file=write_file)\n",
    "    for test_id in range(len(test)):\n",
    "            user_id = test[\"UserId\"][test_id] - 1\n",
    "            find_film = test[\"FilmId\"][test_id] - 1\n",
    "            recommend_score_orig.append(predict_orig[user_id, find_film])\n",
    "            cur_score = predict[user_id, find_film]\n",
    "            recommend_score.append(cur_score)\n",
    "            print(f\"{test_id + 1},{cur_score}\", file=write_file)\n",
    "            \n",
    "recommend_score = np.array(recommend_score)\n",
    "recommend_score_orig = np.array(recommend_score_orig)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "recommend_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_score = []\n",
    "num_view_list = []\n",
    "final_top = []\n",
    "cosine_similarity_items = cosine_similarity(item_vecs)\n",
    "\n",
    "with open(\"submission_als1.csv\", 'w') as write_file:\n",
    "    print(\"Id,Score\", file=write_file)\n",
    "    for test_id in range(len(test)):\n",
    "            user_id = test[\"UserId\"][test_id]\n",
    "            find_film = test[\"FilmId\"][test_id]            \n",
    "            scores = cosine_similarity_items[find_film - 1, :]\n",
    "            scores = scores * mask[user_id - 1, :]\n",
    "            num_view = np.sum(scores > 0)\n",
    "            num_view_list.append(num_view)\n",
    "            if num_view == 0:\n",
    "                cur_score = 5\n",
    "                final_top.append([5])   \n",
    "            else:\n",
    "                max_sim = np.argsort(-scores)[:min(num_view, 10)]\n",
    "                cur_score = np.mean(R[user_id - 1, max_sim])\n",
    "                final_top.append(R[user_id - 1, max_sim])\n",
    "            final_score.append(cur_score)\n",
    "            \n",
    "            print(f\"{test_id + 1},{cur_score}\", file=write_file)\n",
    "final_score = np.array(final_score)\n",
    "num_view_list = np.array(num_view_list)\n",
    "predictions = np.array(final_score)\n",
    "\n",
    "np.sqrt(np.mean((predictions-targets)**2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_score = []\n",
    "num_view_list = []\n",
    "with open(\"submission_als1.csv\", 'w') as write_file:\n",
    "    print(\"Id,Score\", file=write_file)\n",
    "    for test_id in range(len(test)):\n",
    "            user_id = test[\"UserId\"][test_id]\n",
    "            find_film = test[\"FilmId\"][test_id]            \n",
    "            scores = cosine_similarity_items[find_film - 1, :]\n",
    "            scores = scores * mask[user_id - 1, :]\n",
    "            num_view = np.sum(scores != 0)\n",
    "            num_view_list.append(num_view)\n",
    "            if num_view == 0:\n",
    "                 cur_score = 5\n",
    "            else:\n",
    "                max_sim = np.argsort(-scores)[:min(num_view, num_view//10)]\n",
    "                cur_score = np.mean(R[user_id - 1, max_sim])\n",
    "            final_score.append(cur_score)     \n",
    "            print(f\"{test_id + 1},{cur_score}\", file=write_file)\n",
    "final_score = np.array(final_score)\n",
    "num_view_list = np.array(num_view_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_vecs = user_vecs.toarray()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the vector norms\n",
    "final_score_users = []\n",
    "cosine_similarity_users = cosine_similarity(user_vecs)\n",
    "scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "final_top_users = []\n",
    "\n",
    "with open(\"submission_als1.csv\", 'w') as write_file:\n",
    "    print(\"Id,Score\", file=write_file)\n",
    "    for test_id in range(len(test)):\n",
    "            user_id = test[\"UserId\"][test_id] -1 \n",
    "            find_film = test[\"FilmId\"][test_id] - 1            \n",
    "            scores = cosine_similarity_users[user_id, :]\n",
    "            scores = scores * mask[:, find_film]\n",
    "            num_view = np.sum(scores > 0)\n",
    "            if num_view == 0:\n",
    "                cur_score = 5\n",
    "                final_top_users.append([5])   \n",
    "            else:\n",
    "                max_sim = np.argsort(-scores)[:min(num_view, 30)]\n",
    "                cur_score = np.mean(R[max_sim, find_film])\n",
    "                final_top_users.append(R[max_sim, find_film])\n",
    "            final_score_users.append(cur_score)\n",
    "            \n",
    "final_score_users = np.array(final_score_users)\n",
    "\n",
    "predictions = final_score_users\n",
    "\n",
    "np.sqrt(np.mean((predictions-targets)**2))            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the vector norms\n",
    "final_score_users = []\n",
    "cosine_similarity_users = cosine_similarity(user_vecs)\n",
    "scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "\n",
    "\n",
    "with open(\"submission_als1.csv\", 'w') as write_file:\n",
    "    print(\"Id,Score\", file=write_file)\n",
    "    for test_id in range(len(test)):\n",
    "            user_id = test[\"UserId\"][test_id] -1 \n",
    "            find_film = test[\"FilmId\"][test_id] - 1            \n",
    "            scores = cosine_similarity_users[user_id, :]\n",
    "            scores = scores[mask[:, find_film] > 0]\n",
    "            num_view = len(scores)\n",
    "            if num_view == 0:\n",
    "                 cur_score = 5\n",
    "            else:\n",
    "                if num_view > 1:\n",
    "                    scores =  scaler.fit_transform(scores.reshape(-1, 1))\n",
    "                cur_score = np.mean(R[mask[:, find_film] > 0, find_film] * scores)\n",
    "            final_score_users.append(cur_score)     \n",
    "            print(f\"{test_id + 1},{cur_score}\", file=write_file)\n",
    "            \n",
    "final_score_users = np.array(final_score_users)\n",
    "\n",
    "predictions = final_score_users\n",
    "\n",
    "np.sqrt(np.mean((predictions-targets)**2))  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_score_users"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores[mask[:, find_film] > 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cosine_similarity_users[0, :].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "R[mask[:, find_film] > 0, find_film]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_view_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted(final_top, key=lambda x: x[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df = pd.DataFrame()\n",
    "df['final_score'] = final_score\n",
    "df['target'] = targets\n",
    "df['diff_1'] = final_score - targets\n",
    "df['num_view'] = num_view_list\n",
    "df['recommend_score'] = recommend_score\n",
    "df['diff_2'] = recommend_score - targets\n",
    "df['recommend_score_orig'] = recommend_score_orig\n",
    "df['final_score_users'] = final_score_users\n",
    "df['diff_3'] = final_score_users - targets\n",
    "#df['cross_score'] = 3/4 * final_score + 1/4 * recommend_score\n",
    "df['final_top_items'] = final_top\n",
    "df['final_top_users'] = final_top_users\n",
    "df['top_items_mark'] = [item[0] for item in final_top]\n",
    "df['top_users_mark'] = [item[0] for item in final_top_users]\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.sort_values(by='diff_1', ascending=False).head(100)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.loc[(df['top_users_mark'] == 1) & (df['target'] != 1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.loc[(df['top_items_mark'] == 1) & (df['target'] != 1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.loc[df['top_items_mark'].isin([1, 2]) & df['top_users_mark'].isin([1, 2]) & (df['final_score'] < 2.5)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[item[0] for item in final_top_users]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cross_score = 1/2 * final_score + 1/6 * final_score_users + 1/6 * np.array(df['top_items_mark']) + 1/6 * np.array(df['top_users_mark'])\n",
    "predictions = cross_score\n",
    "np.sqrt(np.mean((predictions-targets)**2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cross_score = 2/3 * final_score + 1/9 * final_score_users + 1/9 * np.array(df['top_items_mark']) + 1/9 * np.array(df['top_users_mark'])\n",
    "predictions = cross_score\n",
    "np.sqrt(np.mean((predictions-targets)**2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cross_score = 1/2 * final_score + 1/4 * final_score_users + 1/8 * np.array(df['top_items_mark']) + 1/8 * np.array(df['top_users_mark'])\n",
    "predictions = cross_score\n",
    "np.sqrt(np.mean((predictions-targets)**2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cross_score = 1/2 * final_score + 1/4 * final_score_users + 1/9 * np.array(df['top_items_mark']) + 1/9 * np.array(df['top_users_mark'])\n",
    "cross_score = cross_score + 1/36 * np.array(df['recommend_score'])\n",
    "predictions = cross_score\n",
    "np.sqrt(np.mean((predictions-targets)**2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "\n",
    "X = np.c_[final_score, final_score_users, np.array(df['top_items_mark']), np.array(df['top_users_mark']), np.array(df['recommend_score'])]\n",
    "y = targets\n",
    "\n",
    "reg = LinearRegression().fit(X, y)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reg.get_params()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reg.score(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reg.coef_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reg.intercept_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cross_score = reg.predict(X)\n",
    "predictions = cross_score\n",
    "np.sqrt(np.mean((predictions-targets)**2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.array(df['top_items_mark'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.array(df['top_users_mark'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cosine_similarity_users = cosine_similarity(user_vecs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_score = []\n",
    "\n",
    "with open(\"submission_als1.csv\", 'w') as write_file:\n",
    "    print(\"Id,Score\", file=write_file)\n",
    "    for test_id in range(len(test)):\n",
    "            user_id = test[\"UserId\"][test_id]\n",
    "            find_film = test[\"FilmId\"][test_id]            \n",
    "            scores = cosine_similarity_users[user_id - 1, :]\n",
    "            scores = scores * mask[:, find_film - 1]\n",
    "            num_view = np.sum(scores > 0)\n",
    "            if num_view == 0:\n",
    "                 cur_score = 5\n",
    "            else:\n",
    "                max_sim = np.argsort(-scores)[:min(num_view, 10)]\n",
    "                cur_score = np.mean(R[user_id - 1, max_sim])\n",
    "            final_score.append(cur_score)     \n",
    "            print(f\"{test_id + 1},{cur_score}\", file=write_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_score = np.array(final_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "targets = np.array(targets) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "decpart = np.modf(final_score)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "decpart[decpart > 0.9] = 1\n",
    "decpart[decpart < 0.1] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = np.modf(final_score)[1] + decpart\n",
    "\n",
    "np.sqrt(np.mean((predictions-targets)**2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.modf([0, 3.5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "predictions = np.array(final_score)\n",
    "\n",
    "targets = right_res\n",
    "\n",
    "np.sqrt(np.mean((predictions-targets)**2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = recommend_score\n",
    "np.sqrt(np.mean((predictions-targets)**2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = cross_score\n",
    "np.sqrt(np.mean((predictions-targets)**2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions13"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.sum(predictions13 < 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.sum(np.array(targets) < 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted(-np.abs(predictions13 - targets))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_vecs = user_vecs.toarray()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_vecs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the vector norms\n",
    "user_norms = np.sqrt((user_vecs * user_vecs).sum(axis=1))\n",
    "final_score = []\n",
    "\n",
    "with open(\"submission_als1.csv\", 'w') as write_file:\n",
    "    print(\"Id,Score\", file=write_file)\n",
    "    for test_id in range(len(test)):\n",
    "            user_id = test[\"UserId\"][test_id]\n",
    "            find_film = test[\"FilmId\"][test_id]            \n",
    "            scores = user_vecs.dot(user_vecs[user_id - 1].T) / user_norms\n",
    "            scores = scores * mask[user_id - 1, :]\n",
    "            num_view = np.sum(scores > 0)\n",
    "            if num_view == 0:\n",
    "                 cur_score = 5\n",
    "            else:\n",
    "                max_sim = np.argsort(-scores)[:min(num_view, 10)]\n",
    "                cur_score = np.mean(R[user_id - 1, max_sim])\n",
    "            final_score.append(cur_score)     \n",
    "            print(f\"{test_id + 1},{cur_score}\", file=write_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "\n",
    "with open(\"submission_als1.csv\", 'w') as write_file:\n",
    "    print(\"Id,Score\", file=write_file)\n",
    "    for test_id in range(len(test)):\n",
    "            user_id = test[\"UserId\"][test_id] - 1\n",
    "            recommended = user_vecs[user_id,:].dot(item_vecs.T)\n",
    "            find_film = test[\"FilmId\"][test_id] - 1\n",
    "            recommended = recommended / LA.norm(recommended) \n",
    "            cur_score = recommended[find_film]\n",
    "            print(f\"{test_id + 1},{cur_score}\", file=write_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_id = 0\n",
    "recommended = user_vecs[user_id,:].dot(item_vecs.T)\n",
    "\n",
    "recommended[19]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "item_vecs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "\n",
    "with open(\"submission_als6.csv\", 'w') as write_file:\n",
    "    print(\"Id,Score\", file=write_file)\n",
    "    for test_id in range(len(test)):\n",
    "            user_id = test[\"UserId\"][test_id]\n",
    "            find_film = test[\"FilmId\"][test_id]            \n",
    "            similar = model.similar_items(find_film, n_items)\n",
    "            df = pd.DataFrame(similar, columns =['Id', 'Score'])\n",
    "            df[['Score']] = scaler.fit_transform(df[['Score']])\n",
    "            cur_scores = df['Score'].values * mask[user_id - 1, :]\n",
    "            weights = R[user_id - 1, :]\n",
    "            cur_score = np.sum(cur_scores * weights) / np.sum(weights)\n",
    "            print(f\"{test_id + 1},{cur_score}\", file=write_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "\n",
    "with open(\"submission_als7.csv\", 'w') as write_file:\n",
    "    print(\"Id,Score\", file=write_file)\n",
    "    for test_id in range(len(test)):\n",
    "            user_id = test[\"UserId\"][test_id]\n",
    "            find_film = test[\"FilmId\"][test_id]            \n",
    "            similar = model.similar_users(user_id, n_users)\n",
    "            df = pd.DataFrame(similar, columns =['Id', 'Score'])\n",
    "            df[['Score']] = scaler.fit_transform(df[['Score']])\n",
    "            cur_scores = df['Score'].values * mask[:, find_film - 1]\n",
    "            #weights = R[user_id - 1, :]\n",
    "            max_sim = np.argsort(-cur_scores)[:10]\n",
    "            #cur_score = np.dot(cur_score,weights)\n",
    "            cur_score = np.mean(R[user_id - 1, max_sim])\n",
    "            print(f\"{test_id + 1},{cur_score}\", file=write_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "min(mask.sum(axis=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "user_id = 1\n",
    "find_film = 20\n",
    "similar = model.similar_items(find_film, n_items)\n",
    "\n",
    "df = pd.DataFrame(similar, columns =['Id', 'Score'])\n",
    "df[['Score']] = scaler.fit_transform(df[['Score']])\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cur_scores = df['Score'].values * mask[user_id - 1, :]\n",
    "weights = R[user_id - 1, :]\n",
    "cur_score = np.sum(cur_scores * weights) / np.sum(weights)\n",
    "cur_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cur_score = np.mean(R[user_id - 1, max_sim])\n",
    "#cur_score = np.dot(cur_score,weights)\n",
    "#cur_score = R[user_id - 1, max_sim]\n",
    " \n",
    "cur_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cur_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mask[0, :] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cur_score = df['Score'].values * mask[0, :] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weights = R[0, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.argmax(cur_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.dot(cur_score,weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sum(weights > 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alg_res = pd.read_csv('/Users/lmac/Documents/inf_search/submission_als6.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res1 = pd.read_csv('/Users/lmac/Documents/inf_search/submission_als.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "diff = res1['Score'] - res2['Score']\n",
    "diff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.square(np.sum(diff**2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.sort(diff)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(recommended, columns =['Id', 'Score'])\n",
    "\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max(df['Score'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "min(df['Score'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "R"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mask = np.round(R > 0)\n",
    "\n",
    "mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "R.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_users, n_items"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matrix_completion import svt_solve, calc_unobserved_rmse\n",
    "\n",
    "U = np.random.randn(n_users, 100)\n",
    "V = np.random.randn(n_items, 100)\n",
    "R_hat = svt_solve(R, mask)\n",
    "\n",
    "print(\"RMSE:\", calc_unobserved_rmse(U, V, R_hat, mask))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "U"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "V"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "V.dot(U.transpose())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "R"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import math\n",
    "\n",
    "import numpy as np\n",
    "from scipy.linalg import orth\n",
    "\n",
    "\n",
    "def _make_lr_matrix(m, n, k):\n",
    "    L = np.random.randn(m, k)\n",
    "    R = np.random.randn(k, n)\n",
    "    return np.dot(L, R)\n",
    "\n",
    "\n",
    "def _get_masked_matrix(M, omega):\n",
    "    M_max, M_min = np.max(M), np.min(M)\n",
    "    M_ = M.copy()\n",
    "    M_[(1 - omega).astype(np.int16)] = M_max * M_max\n",
    "    return M_\n",
    "\n",
    "\n",
    "def _get_V_from_U(M, U, omega):\n",
    "    column = M.shape[1]\n",
    "    rank = U.shape[1]\n",
    "    V = np.empty((rank, column), dtype=M.dtype)\n",
    "\n",
    "    for j in range(0, column):\n",
    "        U_ = U.copy()\n",
    "        U_[(1 - omega[:, j]).astype(np.int16), :] = 0\n",
    "        V[:, j] = np.linalg.lstsq(U_, M[:, j], rcond=None)[0]\n",
    "    return V\n",
    "\n",
    "\n",
    "def _get_err(M, U, V, omega):\n",
    "    error_matrix = M - np.dot(U, V)\n",
    "    error_matrix[(1 - omega).astype(np.int16)] = 0\n",
    "    return np.linalg.norm(error_matrix, 'fro') / np.count_nonzero(omega)\n",
    "\n",
    "\n",
    "def _split_omega(omega, T):\n",
    "    omegas = [np.zeros(omega.shape) for t in range(2 * T + 1)]\n",
    "    row, col = omega.shape\n",
    "    for i in range(row):\n",
    "        for j in range(col):\n",
    "            idx = random.randint(0, 2 * T)\n",
    "            omegas[idx][i, j] = omega[i, j]\n",
    "    return omegas\n",
    "\n",
    "\n",
    "def _init_U(M, omega, p, k, mu):\n",
    "    M[(1 - omega).astype(np.int16)] = 0\n",
    "    M = M / p\n",
    "    U, S, V = np.linalg.svd(M, full_matrices=False)\n",
    "    print(U.shape)\n",
    "    U_hat = U.copy()\n",
    "    print(U_hat.shape)\n",
    "    clip_threshold = 2 * mu * math.sqrt(k / max(M.shape))\n",
    "    U_hat[U_hat > clip_threshold] = 0\n",
    "    print(U_hat.shape)\n",
    "    U_hat = orth(U_hat)\n",
    "    print(U_hat.shape)\n",
    "    print(\"|U_hat-U|_F/|U|_F:\",\n",
    "          np.linalg.norm(np.subtract(U_hat, U), ord='fro') / np.linalg.norm(\n",
    "              U, ord='fro'))\n",
    "    return U_hat\n",
    "\n",
    "\n",
    "def _solve(M, omega, p, k, T, mu):\n",
    "    omegas = _split_omega(omega, T)\n",
    "    U = _init_U(M[:, :], omegas[0], p, k, mu)\n",
    "    print('')\n",
    "    V = None\n",
    "    for t in range(T):\n",
    "        V = _get_V_from_U(M, U, omegas[t + 1])\n",
    "        U = _get_V_from_U(M.T, V.T, omegas[T + t + 1].T).T\n",
    "\n",
    "        err = _get_err(M, U, V, omega)\n",
    "        print('>> t(%3d):' % t, err)\n",
    "    print('')\n",
    "    assert V is not None\n",
    "    return np.dot(U, V)\n",
    "\n",
    "\n",
    "def main(M, omega, m, n, k, p, T, mu):\n",
    "    #M = _make_lr_matrix(m, n, k)\n",
    "    #omega = np.zeros((m, n))\n",
    "    #omega[np.random.rand(m, n) <= p] = 1\n",
    "    cardinality_of_omega = np.count_nonzero(omega)\n",
    "    omega = omega.astype(np.int16)\n",
    "    M_rank = np.linalg.matrix_rank(M)\n",
    "    print(\"RANK of M        :\", M_rank)\n",
    "    M_ = _get_masked_matrix(M, omega)\n",
    "\n",
    "    X = _solve(M, omega, p, k, T, mu)\n",
    "    X_rank = np.linalg.matrix_rank(X)\n",
    "    print(\"RANK of X        :\", X_rank)\n",
    "\n",
    "    E = np.subtract(M, X)\n",
    "    E_train = E.copy()\n",
    "    np.place(E_train, 1 - omega, 0)\n",
    "    print('TRAIN RMSE       :',\n",
    "          np.linalg.norm(E_train, \"fro\") / cardinality_of_omega)\n",
    "    E_test = E.copy()\n",
    "    np.place(E_test, omega, 0)\n",
    "    print('TEST  RMSE       :',\n",
    "          np.linalg.norm(E_test, \"fro\") / (m * n - cardinality_of_omega))\n",
    "\n",
    "    print(\"|X-M|_F/|M|_F    :\",\n",
    "          np.linalg.norm(np.subtract(M, X), ord='fro') / np.linalg.norm(\n",
    "              M, ord='fro'))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    # Given\n",
    "    m = n_users\n",
    "    n = n_items\n",
    "    p = 0.1\n",
    "\n",
    "    # Hyper Parameters\n",
    "    k = 40\n",
    "    T = 5\n",
    "    mu = 0.1\n",
    "    main(R, mask, m, n, k, p, T, mu)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from d2l import mxnet as d2l\n",
    "import os\n",
    "import pandas as pd\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@save\n",
    "d2l.DATA_HUB['ml-100k'] = (\n",
    "    'http://files.grouplens.org/datasets/movielens/ml-100k.zip',\n",
    "    'cd4dcac4241c8a4ad7badc7ca635da8a69dddb83')\n",
    "\n",
    "#@save\n",
    "def read_data_ml100k():\n",
    "    data_dir = d2l.download_extract('ml-100k')\n",
    "    names = ['user_id', 'item_id', 'rating', 'timestamp']\n",
    "    data = pd.read_csv(os.path.join(data_dir, 'u.data'), '\\t', names=names,\n",
    "                       engine='python')\n",
    "    num_users = data.user_id.unique().shape[0]\n",
    "    num_items = data.item_id.unique().shape[0]\n",
    "    return data, num_users, num_items"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data, num_users, num_items = read_data_ml100k()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mldat = np.zeros((num_users, num_items))\n",
    "\n",
    "for row in data.itertuples():\n",
    "    mldat[(row[1]-1), (row[2]-1)] = row[3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mldat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mldat.item((0, 0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "right_res = []\n",
    "for test_id in range(len(test)):\n",
    "            user_id = test[\"UserId\"][test_id] - 1\n",
    "            find_film = test[\"FilmId\"][test_id] - 1\n",
    "            right_res.append(mldat.item((user_id, find_film)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "right_res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "right_res = np.array(right_res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "right_res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alg_res13 = pd.read_csv('/Users/lmac/Documents/inf_search/submission_als1.csv')\n",
    "\n",
    "predictions13 = alg_res13['Score'].to_numpy()\n",
    "\n",
    "targets = right_res\n",
    "\n",
    "np.sqrt(np.mean((predictions13-targets)**2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.argsort(predictions-targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test.values[6749-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted(predictions-targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions5-targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max(predictions11-targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "min(predictions11-targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = predictions[np.argsort(predictions-targets)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions[predictions != 5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "right_res[len(right_res) - 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.sum(R[:, 1254] > 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, sys, numpy as np\n",
    "\n",
    "\n",
    "os.environ[\"PYSPARK_PYTHON\"]=\"/usr/local/bin/python3\"\n",
    "os.environ[\"PYSPARK_DRIVER_PYTHON\"]=\"/usr/local/bin/python3\"\n",
    "os.environ['SPARK_HOME']=\"/Users/lmac/Downloads/spark-3.0.1-bin-hadoop2.7\"\n",
    "sys.path.append(\"/Users/lmac/Downloads/spark-3.0.1-bin-hadoop2.7/python/\")\n",
    "\n",
    "\n",
    "from pyspark import SparkConf, SparkContext\n",
    "conf = (SparkConf().setMaster(\"local\").setAppName(\"My app\").set(\"spark.executor.memory\", \"1g\"))\n",
    "from pyspark.sql import SQLContext\n",
    "from pyspark.sql.dataframe import StructType, StructField, IntegerType, FloatType\n",
    "sc = SparkContext(conf = conf)\n",
    "sqlContext = SQLContext(sc)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mv_ratings = sc.textFile('/Users/lmac/Documents/inf_search/movie-recomendation-fall-2020/train.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mv_ratings.first()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "item_user_ratings.first()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating sparse representation of A matrix with users as rows and items as columns\n",
    "user_item_ratings = mv_ratings.map(lambda line: (int(line.split('\\t')[0]), (int(line.split('\\t')[1]), line.split('\\t')[2])))\n",
    "user_item_ratings = user_item_ratings.groupByKey()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "item_user_ratings = mv_ratings.map(lambda line: (int(line.split('\\t')[1]), (int(line.split('\\t')[0]), line.split('\\t')[2])))\n",
    "item_user_ratings = item_user_ratings.groupByKey()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lambda_ = sc.broadcast(0.1) # Regularization parameter\n",
    "n_factors = sc.broadcast(3) # nfactors of User matrix and Item matrix\n",
    "n_iterations = 20 # How many times to iterate over the user and item matrix calculations.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Items = item_user_ratings.map(lambda line: (line[0], 5 * np.random.rand(1, n_factors.value)))\n",
    "print(Items.take(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Items_broadcast = sc.broadcast({\n",
    "  k: v for (k, v) in Items.collect()\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Number of Items (columns of A matrix):\", item_user_ratings.count())\n",
    "print(\"Number of Users (rows of A matrix):\", user_item_ratings.count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Update_User(userTuple):\n",
    "\n",
    "    Itemssquare = np.zeros([n_factors.value,n_factors.value])\n",
    "    for matrixA_item_Tuple in userTuple[1]:\n",
    "        itemRow = Items_broadcast.value[matrixA_item_Tuple[0]][0]\n",
    "        for i in range(n_factors.value):\n",
    "            for j in range(n_factors.value):\n",
    "                Itemssquare[i,j] += float(itemRow[i]) * float(itemRow[j])\n",
    "    leftMatrix = np.linalg.inv(Itemssquare + lambda_.value * np.eye(n_factors.value))\n",
    "    rightMatrix = np.zeros([1,n_factors.value])\n",
    "    for matrixA_item_Tuple in userTuple[1]:\n",
    "        for i in range(n_factors.value):\n",
    "            rightMatrix[0][i] += Items_broadcast.value[matrixA_item_Tuple[0]][0][i] * float(matrixA_item_Tuple[1])\n",
    "    newUserRow = np.dot(leftMatrix, rightMatrix.T).T\n",
    "    return (userTuple[0], newUserRow)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Users = user_item_ratings.map(Update_User)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The item matrix is needed in all partitions when solving for rows of User matrix individually\n",
    "Users_broadcast = sc.broadcast({\n",
    "  k: v for (k, v) in Users.collect()\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Update_Item(itemTuple):\n",
    "\n",
    "    Userssquare = np.zeros([n_factors.value,n_factors.value])\n",
    "    for matrixA_user_Tuple in itemTuple[1]:\n",
    "        userRow = Users_broadcast.value[matrixA_user_Tuple[0]][0]\n",
    "        for i in range(n_factors.value):\n",
    "            for j in range(n_factors.value):\n",
    "                Userssquare[i,j] += float(userRow[i]) * float(userRow[j])\n",
    "    leftMatrix = np.linalg.inv(Userssquare + lambda_.value * np.eye(n_factors.value))\n",
    "    rightMatrix = np.zeros([1,n_factors.value])\n",
    "    for matrixA_user_Tuple in itemTuple[1]:\n",
    "        for i in range(n_factors.value):\n",
    "            rightMatrix[0][i] += Users_broadcast.value[matrixA_user_Tuple[0]][0][i] * float(matrixA_user_Tuple[1])\n",
    "    newItemRow = np.dot(leftMatrix, rightMatrix.T).T\n",
    "    return (itemTuple[0], newItemRow)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Items = item_user_ratings.map(Update_Item)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Items_broadcast = sc.broadcast({\n",
    "  k: v for (k, v) in Items.collect()\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getRowSumSquares(userTuple):\n",
    "    userRow = Users_broadcast.value[userTuple[0]]\n",
    "    rowSSE = 0.0\n",
    "    for matrixA_item_Tuple in userTuple[1]:\n",
    "        predictedRating = 0.0\n",
    "        for i in range(n_factors.value):\n",
    "            predictedRating += userRow[0][i] * Items_broadcast.value[matrixA_item_Tuple[0]][0][i]\n",
    "        SE = (float(matrixA_item_Tuple[1]) - predictedRating) ** 2\n",
    "        rowSSE += SE\n",
    "    return rowSSE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SSE = user_item_ratings.map(getRowSumSquares).reduce(lambda a, b: a + b)\n",
    "Count = mv_ratings.count()\n",
    "MSE = SSE / Count\n",
    "print(\"MSE:\", MSE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for iter in range(n_iterations):\n",
    "    Users = user_item_ratings.map(Update_User)\n",
    "    Users_broadcast = sc.broadcast({k: v for (k, v) in Users.collect()})\n",
    "    Items = item_user_ratings.map(Update_Item)\n",
    "    Items_broadcast = sc.broadcast({k: v for (k, v) in Items.collect()})\n",
    "    SSE = user_item_ratings.map(getRowSumSquares).reduce(lambda a, b: a + b)\n",
    "    MSE = SSE / Count\n",
    "    print(\"MSE:\", MSE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Users.first()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Items.first()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Final version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "\n",
    "np.random.seed(100)\n",
    "from matplotlib import pyplot as plt\n",
    "class ImplicitALS:\n",
    "    \n",
    "    def __init__(self, max_epoch=10, embedding_size=15, alpha=10, lamdda=0.1, eps=0.1,\n",
    "                  mean_decrease=0.85):\n",
    "\n",
    "        self.max_epoch = max_epoch\n",
    "        self.embedding_size = embedding_size\n",
    "        self.alpha = alpha\n",
    "        self.lamdda = lamdda\n",
    "        self.eps = eps\n",
    "        self.mean_decrease = mean_decrease\n",
    "\n",
    "    def log_confidence(self, R_train):\n",
    "        return (np.ones(R_train.shape) + self.alpha * np.log(np.ones(R_train.shape) + R_train / self.eps))\n",
    "\n",
    "    def normalisation(self, R, R_test):\n",
    "        gr_mask = R > 0\n",
    "        eq_mask = R == 0\n",
    "        mean = R[gr_mask].mean()\n",
    "        R_unbiased = R * gr_mask - gr_mask * mean\n",
    "        user_bias = (R_unbiased.sum(1) / gr_mask.sum(1)).reshape(-1, 1)\n",
    "        R_unuserbiased = R_unbiased * gr_mask - gr_mask * user_bias\n",
    "        gr_mask_sum_0 = gr_mask.sum(0)\n",
    "        gr_mask_sum_0[gr_mask_sum_0 == 0] = 1\n",
    "        item_bias = (R_unuserbiased.sum(0) / gr_mask_sum_0).reshape(1, -1)\n",
    "\n",
    "        gr_mask_test = R_test > 0\n",
    "        P = R + eq_mask * (~gr_mask_test) * (user_bias + item_bias + mean) * self.mean_decrease + eq_mask * \\\n",
    "            gr_mask_test * (user_bias + item_bias + mean)\n",
    "        return P, mean, gr_mask, eq_mask\n",
    "\n",
    "    def calc_loss(self, epoch, result, R_train, R_test, X, Y, C, gamma, beta, mean, gr_mask):\n",
    "\n",
    "        train_error = np.sqrt(((result * gr_mask - R_train) ** 2).sum() / gr_mask.sum())\n",
    "\n",
    "        R = R_train.copy()\n",
    "        if R_test is not None:\n",
    "            test_error = np.sqrt(((result * (R_test > 0) - R_test) ** 2).sum() / (R_test > 0).sum())\n",
    "            R += R_test\n",
    "        else:\n",
    "            test_error = None\n",
    "\n",
    "        metric = 1.0/(R.shape[0] * R.shape[1]) * \\\n",
    "                 (C * (R - beta - gamma - result) ** 2).sum() + \\\n",
    "                 self.lamdda * ((X**2).sum() + (Y**2).sum() + (beta**2).sum() + (gamma**2).sum())\n",
    "        return train_error, test_error, metric\n",
    "\n",
    "    def fit(self, R_train, R_test=None):\n",
    "\n",
    "        tr_loss = []\n",
    "        te_loss = []\n",
    "        losses = []\n",
    "        P, mean, gr_mask, eq_mask = self.normalisation(R_train, R_test)\n",
    "\n",
    "        C = self.log_confidence(R_train)\n",
    "\n",
    "        laI = np.eye(self.embedding_size + 1, self.embedding_size + 1) * self.lamdda  \n",
    "\n",
    "\n",
    "        X = np.hstack([np.ones((R_train.shape[0], 1)), np.random.random_sample((R_train.shape[0], self.embedding_size))])\n",
    "        Y = np.hstack([np.ones((R_train.shape[1], 1)), np.random.random_sample((R_train.shape[1], self.embedding_size))])\n",
    "\n",
    "        beta = np.zeros((X.shape[0], 1))  \n",
    "        gamma = np.zeros((X.shape[0], 1))  \n",
    "\n",
    "\n",
    "        n_user = (R_train + R_test > 0).sum(1)\n",
    "        n_item = (R_train + R_test > 0).sum(0)\n",
    "\n",
    "\n",
    "        for epoch in range(self.max_epoch):\n",
    "\n",
    "            Pgamma = P - gamma\n",
    "            Cp = C * Pgamma\n",
    "            for i in range(X.shape[0]): \n",
    "                YTCuY_laI = np.dot(C[i,:] * Y.T, Y) + laI * (n_user[i]) \n",
    "                YTCuY_laI_inv = np.linalg.inv(YTCuY_laI)\n",
    "                YTCuY_laI_inv_YT = np.matmul(YTCuY_laI_inv, Y.T)\n",
    "                X[i, :] = np.matmul(YTCuY_laI_inv_YT, Cp[i, :].reshape(-1, 1)).ravel()\n",
    "\n",
    "            beta = X[:, 0].copy().reshape(-1, 1)\n",
    "            X[:, 0] = 1\n",
    "            Pbeta = P - beta\n",
    "            Cp = C * Pbeta\n",
    "            for j in range(Y.shape[0]):  \n",
    "                \n",
    "                XTCuX_laI = np.dot(C[:,j] * X.T, X) + laI * (n_item[j]) \n",
    "                XTCuX_laI_inv = np.linalg.inv(XTCuX_laI)\n",
    "                XTCuX_laI_inv_XT = np.matmul(XTCuX_laI_inv, X.T)\n",
    "                Y[j, :] = np.matmul(XTCuX_laI_inv_XT, Cp[:, j].reshape(-1, 1)).ravel()\n",
    "\n",
    "            gamma = Y[:, 0].copy().reshape(1, -1)\n",
    "            Y[:, 0] = 1\n",
    "            current_result = np.matmul(X[:, 1:], Y[:, 1:].T) + beta + gamma\n",
    "            current_result_new = current_result.copy()\n",
    "\n",
    "            current_result_new[current_result_new > 5] = 5\n",
    "            current_result_new[current_result_new < 1] = 1\n",
    "            \n",
    "            \n",
    "            \n",
    "            train_error, test_error, loss = self.calc_loss(epoch, current_result_new, R_train, R_test, X, Y, C,\n",
    "                              gamma, beta, mean,  gr_mask)\n",
    "            \n",
    "            tr_loss.append(train_error)\n",
    "            te_loss.append(test_error)\n",
    "            losses.append(loss)\n",
    "\n",
    "        return current_result_new, tr_loss, te_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.sparse import csr_matrix\n",
    "import numpy as np\n",
    "\n",
    "train = np.loadtxt('/Users/lmac/Documents/inf_search/movie-recomendation-fall-2020/train.txt',  delimiter=\"\\t\", dtype=int)\n",
    "\n",
    "M = csr_matrix((train[:,2], (train[:,0]-1, train[:,1]-1))).toarray()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(100)\n",
    "def divide_dataset(matrix, ratio=0.1):\n",
    "\n",
    "    pres_elements = np.arange(matrix.shape[0] * matrix.shape[1])[matrix.ravel() > 0]\n",
    "    \n",
    "    test_size = int(ratio * pres_elements.shape[0])\n",
    "    \n",
    "    np.random.shuffle(pres_elements)\n",
    "    \n",
    "    train_el = pres_elements[test_size:]\n",
    "    test_el = pres_elements[:test_size]\n",
    "    \n",
    "    train_ind = np.unravel_index(train_el, matrix.shape)\n",
    "    test_ind = np.unravel_index(test_el, matrix.shape)\n",
    "    \n",
    "    \n",
    "    train_mask = np.zeros(matrix.shape)\n",
    "    test_mask = np.zeros(matrix.shape)\n",
    "    \n",
    "    for i in range(train_ind[0].shape[0]):\n",
    "        train_mask[train_ind[0][i], train_ind[1][i]] = 1\n",
    "    \n",
    "    for i in range(test_ind[0].shape[0]):\n",
    "        test_mask[test_ind[0][i], test_ind[1][i]] = 1\n",
    "        \n",
    "    return matrix * train_mask, matrix * test_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(100)\n",
    "results = []\n",
    "\n",
    "alphas = [25, 30, 35, 40]\n",
    "embedding_sizes = [5, 6]\n",
    "lamddas = [2, 5, 8]\n",
    "eps = [0.05,0.1,0.15]\n",
    "\n",
    "for alph in alphas:\n",
    "    for emb_size in embedding_sizes:\n",
    "        for lam in lamddas:\n",
    "            for e in eps:\n",
    "                print(alph, emb_size, lam, e)\n",
    "                model = ImplicitALS(max_epoch=20,\n",
    "                        embedding_size=emb_size,\n",
    "                        alpha=alph,\n",
    "                        lamdda=lam,\n",
    "                        eps=e,\n",
    "                        mean_decrease=0.85)\n",
    "                M_train, M_v = divide_dataset(M, 0.1)\n",
    "                res, _, _ = model.fit(M_train, M_v)\n",
    "                results.append(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_result = np.zeros(results[0].shape)\n",
    "for r in results:\n",
    "    final_result = final_result + r\n",
    "final_result = final_result/len(results)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = pd.read_csv('/Users/lmac/Documents/inf_search/movie-recomendation-fall-2020/test.txt', header = None, sep = \"\\t\")\n",
    "test.columns = [\"UserId\", \"FilmId\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "with open(\"submission_als_new_try.csv\", 'w') as write_file:\n",
    "    print(\"Id,Score\", file=write_file)\n",
    "    for test_id in range(len(test)):\n",
    "            n_row = test[\"UserId\"][test_id] - 1\n",
    "            n_col = test[\"FilmId\"][test_id] - 1\n",
    "            print(f\"{test_id + 1},{final_result[n_row, n_col]}\", file=write_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
