{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "from sklearn.metrics import log_loss, roc_auc_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder, MinMaxScaler\n",
    "\n",
    "from deepctr_torch.inputs import SparseFeat, DenseFeat, get_feature_names\n",
    "from deepctr_torch.models import *\n",
    "import pandas as pd\n",
    "import csv\n",
    "from tqdm import tqdm_notebook as tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "header = open (\"/Users/lmac/Documents/inf_search/online-advertising-challenge-fall-2020/train_adv/train.csv\").readline ()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "header"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns = header.strip ().split (\";\")\n",
    "columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv (\"/Users/lmac/Documents/inf_search/online-advertising-challenge-fall-2020/train_adv/train.csv\", sep=';', names=columns, skiprows=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['C1'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv (\"data/train_2.csv\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sparse_features = ['C' + str(i) for i in range(1, 30)]\n",
    "dense_features = ['I' + str(i) for i in range(1, 2)]\n",
    "\n",
    "data[sparse_features] = data[sparse_features].fillna('-1', )\n",
    "data[dense_features] = data[dense_features].fillna(0, )\n",
    "target = ['Label']\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1.Label Encoding for sparse features,and do simple Transformation for dense features\n",
    "for feat in sparse_features:\n",
    "        lbe = LabelEncoder()\n",
    "        data[feat] = lbe.fit_transform(data[feat])\n",
    "mms = MinMaxScaler(feature_range=(0, 1))\n",
    "data[dense_features] = mms.fit_transform(data[dense_features])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2.count #unique features for each sparse field,and record dense feature field name\n",
    "\n",
    "fixlen_feature_columns = [SparseFeat(feat, data[feat].nunique())\n",
    "                              for feat in sparse_features] + [DenseFeat(feat, 1, )\n",
    "                                                              for feat in dense_features]\n",
    "\n",
    "dnn_feature_columns = fixlen_feature_columns\n",
    "linear_feature_columns = fixlen_feature_columns\n",
    "\n",
    "feature_names = get_feature_names(linear_feature_columns + dnn_feature_columns)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3.generate input data for model\n",
    "\n",
    "train, test = train_test_split(data, test_size=0.2, random_state=2020)\n",
    "train_model_input = {name: train[name] for name in feature_names}\n",
    "test_model_input = {name: test[name] for name in feature_names}\n",
    "train_model_all = {name: data[name] for name in feature_names}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "np.sum(train_model_input[target].values)\n",
    "len(train_model_input[target].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_model_input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4.Define Model,train,predict and evaluate\n",
    "\n",
    "device = 'cpu'\n",
    "use_cuda = False\n",
    "if use_cuda and torch.cuda.is_available():\n",
    "        print('cuda ready...')\n",
    "        device = 'cuda:0'\n",
    "\n",
    "model = DeepFM(linear_feature_columns=linear_feature_columns, dnn_feature_columns=dnn_feature_columns,\n",
    "                   task='binary',\n",
    "                   l2_reg_embedding=1e-5, device=device)\n",
    "\n",
    "model.compile(\"adagrad\", \"binary_crossentropy\",\n",
    "                  metrics=[\"binary_crossentropy\", \"auc\"], )\n",
    "\n",
    "history = model.fit(train_model_all, data[target].values, batch_size=32, epochs=10, verbose=2,\n",
    "                        validation_split=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import zlib\n",
    "from collections import defaultdict\n",
    "\n",
    "HEADER = \"timestamp;label;C1;C2;C3;C4;C5;C6;C7;C8;C9;C10;CG1;CG2;CG3;l1;l2;C11;C12\".split(';')\n",
    "COUNT_BASIC = 2\n",
    "COUNT_CATEG_FIRST = 10\n",
    "COUNT_GROUP = 3\n",
    "COUNT_CATEG_SECOND = 2\n",
    "COUNT_CATEGORIAL = COUNT_CATEG_FIRST+COUNT_CATEG_SECOND\n",
    "\n",
    "OUT_HEADER = \"timestamp;Label;I1;I2;C1;C2;C3;C4;C5;C6;C7;C8;C9;C10;C11;C12;C13;C14;C15;C16;C17;C18;C19;C20;C21;C22;C23;C24;C25;C26;C27;C28;C29;C30\".split(';')\n",
    "\n",
    "CG1_GROUPS = {\"C13\": [335, 99, 122, 58, 385, 268, 341, 382, 52, 416],\n",
    "              \"C14\": [332, 76, 59, 96, 399, 273, 57, 53, 49, 437],\n",
    "              \"C15\": [334, 155, 422, 130, 74, 357, 419, 276, 54, 343],\n",
    "              \"C16\": [333, 154, 331, 151, 18, 279, 330, 449, 336, 269],\n",
    "              \"C17\": [139, 412, 123, 124, 150, 205, 435, 277, 438, 88],\n",
    "              \"C18\": []} #For other vals\n",
    "\n",
    "CG2_GROUPS = {\"C19\": [16810, 20892, 25731, 8426, 8395, 29347, 18705, 2390, 8833, 823],\n",
    "              \"C20\": [30009, 16444, 22582, 29432, 3864, 4378, 25900, 7326, 31328, 3326],\n",
    "              \"C21\": [2293, 944, 19883, 29463, 6746, 2253, 16676, 7923, 516, 17179],\n",
    "              \"C22\": [18714, 7636, 14322, 10328, 6619, 29768, 15650, 30235, 24843, 18543],\n",
    "              \"C23\": [28695, 9793, 2254, 19225, 20755, 3746, 30749, 24338, 207, 18724],\n",
    "              \"C24\": []}\n",
    "\n",
    "CG3_GROUPS = {\"C25\": [46839, 49517, 49513, 48114, 44289, 57895, 17419, 11599, 56445, 7592],\n",
    "              \"C26\": [38344, 46594, 6214, 49272, 45902, 46401, 24887, 3311, 49784, 37588],\n",
    "              \"C27\": [45509, 75, 15769, 38346, 27636, 28676, 40461, 5178, 19563, 8894],\n",
    "              \"C28\": [56597, 15529, 45501, 18336, 49966, 21144, 47396, 2340, 49518, 8650],\n",
    "              \"C29\": [33701, 54846, 55412, 43844, 43845, 20076, 49962, 43666, 23339, 33789],\n",
    "              \"C30\": []}\n",
    "\n",
    "\n",
    "def ProcessGroup(group_name, stat, group_feature):\n",
    "\n",
    "    vals_array = None\n",
    "    if group_name == 'CG1':\n",
    "        vals_array = CG1_GROUPS\n",
    "    elif group_name == 'CG2':\n",
    "        vals_array = CG2_GROUPS\n",
    "    elif group_name == 'CG3':\n",
    "        vals_array = CG3_GROUPS\n",
    "    else:\n",
    "        raise Exception(\"Unknown group\")\n",
    "\n",
    "    result = defaultdict(list)\n",
    "    for val in group_feature.split(','):\n",
    "        if val == '':\n",
    "            continue\n",
    "        else:\n",
    "            val = int(val)\n",
    "\n",
    "        for key, feature_vals in vals_array.items():\n",
    "            if len(feature_vals) == 0: # orphan\n",
    "                result[key].append(str(val))\n",
    "                break\n",
    "            if val in feature_vals:\n",
    "                result[key].append(str(val))\n",
    "                break\n",
    "    for key in result:\n",
    "        if len(result[key]) == 0:\n",
    "            result[key] = ''\n",
    "        else:\n",
    "            result[key] = str(zlib.crc32((\",\".join(sorted(result[key]))).encode()))\n",
    "        global_stat[key+\"-\"+result[key]] += 1\n",
    "    return result\n",
    "\n",
    "def GetFeatures(input_str, global_statistics):\n",
    "    features = {}\n",
    "    for feature_id, feature_str in enumerate(input_str.split(';')):\n",
    "        feature_name = HEADER[feature_id]\n",
    "        if feature_name == 'label':\n",
    "            feature_name = 'Label'\n",
    "        elif feature_name.startswith('CG'): # group\n",
    "            features.update(ProcessGroup(feature_name, global_stat, feature_str))\n",
    "        elif feature_name.startswith('l'): # counters\n",
    "            feature_name = feature_name.replace('l', 'I')\n",
    "\n",
    "        if not feature_name.startswith('CG'):\n",
    "            features.update({feature_name: feature_str})\n",
    "        if feature_name.startswith('C') and not feature_name.startswith('CG'):\n",
    "            global_statistics[feature_name+\"-\"+feature_str] += 1\n",
    "\n",
    "    return features\n",
    "\n",
    "def CreateOutFeatures(input_str, statistics):\n",
    "    features = GetFeatures(input_str, statistics)\n",
    "    out = \"\"\n",
    "    for feature_name in OUT_HEADER:\n",
    "        feature_val = features.get(feature_name)\n",
    "        if feature_val is None:\n",
    "            feature_val = ''\n",
    "        out += feature_val + \",\"\n",
    "    return out[:-1]\n",
    "\n",
    "def ProcessFile(filename_in, filename_out, global_statistics):\n",
    "    with open(filename_in, 'r', encoding='utf-8') as input, \\\n",
    "            open(filename_out, 'w', encoding='utf-8') as output:\n",
    "        output.write(','.join(OUT_HEADER)+'\\n')\n",
    "        for counter, line in enumerate(input):\n",
    "            if line.startswith('timestamp'):\n",
    "                continue\n",
    "\n",
    "            line = line[:-1]\n",
    "            out_line = CreateOutFeatures(line, global_statistics)\n",
    "            output.write(out_line+'\\n')\n",
    "            if counter % 100000 == 0:\n",
    "                print(filename_in, counter)\n",
    "                #break\n",
    "\n",
    "global_stat = defaultdict(int)\n",
    "ProcessFile('/Users/lmac/Documents/inf_search/online-advertising-challenge-fall-2020/train_adv/train.csv', 'data/train_2.csv', global_stat)\n",
    "ProcessFile('/Users/lmac/Documents/inf_search/online-advertising-challenge-fall-2020/test_adv/test.csv', 'data/test_2.csv', global_stat)\n",
    "\n",
    "sorted_stat = sorted(global_stat.items(), key=lambda kv: -kv[1])\n",
    "with open('data/stat_2.txt', 'w', encoding='utf-8') as stat_file:\n",
    "    for stat_name, stat_count in sorted_stat:\n",
    "        stat_file.write(stat_name+\"\\t\"+str(stat_count)+\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('/Users/lmac/Documents/inf_search/online-advertising-challenge-fall-2020/train_adv/train.csv', newline='') as csvfile, open('data/train_drop_groups.csv', 'w', newline='') as csvout: \n",
    "    spamreader = csv.reader(csvfile, delimiter=';', quotechar='|')       \n",
    "    spamwriter = csv.writer(csvout, delimiter=';',\n",
    "                            quotechar='|', quoting=csv.QUOTE_MINIMAL)\n",
    "    for row in spamreader:\n",
    "        out_row = row[0:12]\n",
    "        out_row.extend(row[15:])\n",
    "        spamwriter.writerow(out_row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('data/train_drop_groups.csv', newline='') as csvfile, open('data/train_libffm', 'wb') as f: \n",
    "    spamreader = csv.reader(csvfile, delimiter=';', quotechar='|')\n",
    "    hdr = next(spamreader)\n",
    "    indices = list(map(str, range(0, len(hdr) - 1)))\n",
    "    c_i = 0\n",
    "    for row in spamreader:\n",
    "        if (row[1] == '1') | (c_i < 1000000):\n",
    "            out_line = row[1] + \" \" \n",
    "            row = [row[0]] + row[2:]\n",
    "            zipper = list(zip(indices, row))\n",
    "            out_line += \" \".join([a + \":1:\" + b for a,b in zipper[0:1]]) + \" \"\n",
    "            out_line += \" \".join([a + \":\" + b + \":1\" for a,b in zipper[1:11]])+ \" \"\n",
    "            out_line += \" \".join([a + \":1:\" + b for a,b in zipper[11:13]])+ \" \"\n",
    "            out_line += \" \".join([a + \":\" + b + \":1\" for a,b in zipper[13:]])+ \" \"\n",
    "            f.write(str.encode(out_line[:-1] + '\\n'))\n",
    "            c_i +=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xlearn as xl\n",
    "\n",
    "ffm_model = xl.create_ffm()\n",
    "ffm_model.setTrain(\"data/train_libffm\")\n",
    "ffm_model.setValidate(\"data/test_libffm_small\")\n",
    "ffm_model.setTest('data/test_libffm')\n",
    "ffm_model.setSigmoid()\n",
    "ffm_model.disableEarlyStop();\n",
    "param = {'task':'binary', 'lr':0.2, 'lambda':0.002, 'epoch':10}\n",
    "\n",
    "ffm_model.fit(param, \"data/model.out\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('/Users/lmac/Documents/inf_search/online-advertising-challenge-fall-2020/test_adv/test.csv', newline='') as csvfile, open('data/test_drop_groups.csv', 'w', newline='') as csvout: \n",
    "    spamreader = csv.reader(csvfile, delimiter=';', quotechar='|')       \n",
    "    spamwriter = csv.writer(csvout, delimiter=';',\n",
    "                            quotechar='|', quoting=csv.QUOTE_MINIMAL)\n",
    "    for row in spamreader:\n",
    "        out_row = row[0:12]\n",
    "        out_row.extend(row[15:])\n",
    "        spamwriter.writerow(out_row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('data/test_drop_groups.csv', newline='') as csvfile, open('data/test_libffm_small', 'wb') as f: \n",
    "    spamreader = csv.reader(csvfile, delimiter=';', quotechar='|')\n",
    "    hdr = next(spamreader)\n",
    "    indices = list(map(str, range(0, len(hdr) - 1)))\n",
    "    c_i = 0\n",
    "    for row in spamreader:\n",
    "        if (c_i < 10000):\n",
    "            out_line = row[1] + \" \" \n",
    "            row = [row[0]] + row[2:]\n",
    "            zipper = list(zip(indices, row))\n",
    "            out_line += \" \".join([a + \":1:\" + b for a,b in zipper[0:1]]) + \" \"\n",
    "            out_line += \" \".join([a + \":\" + b + \":1\" for a,b in zipper[1:11]])+ \" \"\n",
    "            out_line += \" \".join([a + \":1:\" + b for a,b in zipper[11:13]])+ \" \"\n",
    "            out_line += \" \".join([a + \":\" + b + \":1\" for a,b in zipper[13:]])+ \" \"\n",
    "            f.write(str.encode(out_line[:-1] + '\\n'))\n",
    "            c_i += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "header = open (\"data/test_2.csv\").readline ()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "header"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('data/train_2.csv', newline='') as csvfile, open('data/train_libffm', 'wb') as f: \n",
    "    spamreader = csv.reader(csvfile, delimiter=',', quotechar='|')\n",
    "    hdr = next(spamreader)\n",
    "    indices = list(map(str, range(0, len(hdr) - 2)))\n",
    "    c_i = 0\n",
    "    c_sum = 0\n",
    "    for row in spamreader:\n",
    "        if ((row[1] == '1') | (c_i < 1000000)):\n",
    "            out_line = row[1] + \" \" \n",
    "            c_sum += (row[1] == '1')*1\n",
    "            row = row[2:]\n",
    "            zipper = list(zip(indices, row))\n",
    "            out_line += \" \".join([a + \":1:\" + b for a,b in zipper[0:2]]) + \" \"\n",
    "            out_line += \" \".join([a + \":\" + b + \":1\" for a,b in zipper[2:31]])+ \" \"\n",
    "            f.write(str.encode(out_line[:-1] + '\\n'))\n",
    "            c_i = c_i + 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xlearn as xl\n",
    "\n",
    "ffm_model = xl.create_ffm()\n",
    "ffm_model.setTrain(\"data/train_libffm\")\n",
    "#ffm_model.setValidate(\"./small_test.txt\")\n",
    "#ffm_model.disableEarlyStop();\n",
    "param = {'task':'binary', 'lr':0.2, 'lambda':0.002, 'epoch':10}\n",
    "\n",
    "ffm_model.fit(param, \"data/model.out\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "from csv import DictReader\n",
    "from math import exp, log, sqrt\n",
    "\n",
    "\n",
    "# parameters #################################################################\n",
    "\n",
    "train = 'data/train_2.csv'  # path to training file\n",
    "test = 'data/test_2.csv'  # path to testing file\n",
    "\n",
    "D = 2 ** 20   # number of weights use for learning\n",
    "alpha = .1    # learning rate for sgd optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def logloss(p, y):\n",
    "    p = max(min(p, 1. - 10e-12), 10e-12)\n",
    "    return -log(p) if y == 1. else -log(1. - p)\n",
    "\n",
    "\n",
    "\n",
    "def get_x(csv_row, D):\n",
    "    x = [0]  # 0 is the index of the bias term\n",
    "    for key, value in csv_row.items():\n",
    "        index = int(value + key[1:], 31) % D  \n",
    "        x.append(index)\n",
    "    return x  # x contains indices of features that have a value of 1\n",
    "\n",
    "\n",
    "\n",
    "def get_p(x, w):\n",
    "    wTx = 0.\n",
    "    for i in x:  # do wTx\n",
    "        wTx += w[i] * 1.  # w[i] * x[i], but if i in x we got x[i] = 1.\n",
    "    return 1. / (1. + exp(-max(min(wTx, 20.), -20.)))  # bounded sigmoid\n",
    "\n",
    "\n",
    "\n",
    "def update_w(w, n, x, p, y):\n",
    "    for i in x:\n",
    "        # alpha / (sqrt(n) + 1) is the adaptive learning rate heuristic\n",
    "        # (p - y) * x[i] is the current gradient\n",
    "        # in our case, if i in x then x[i] = 1\n",
    "        w[i] -= (p - y) * alpha / (sqrt(n[i]) + 1.)\n",
    "        n[i] += 1.\n",
    "\n",
    "    return w, n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# training and testing #######################################################\n",
    "\n",
    "# initialize our model\n",
    "w = [0.] * D  # weights\n",
    "n = [0.] * D  # number of times we've encountered a feature\n",
    "\n",
    "# start training a logistic regression model using on pass sgd\n",
    "loss = 0.\n",
    "for t, row in enumerate(DictReader(open(train))):\n",
    "    y = 1. if row['Label'] == '1' else 0.\n",
    "\n",
    "    del row['Label']  \n",
    "    del row['timestamp']  \n",
    "\n",
    "    # main training procedure\n",
    "    # step 1, get the hashed features\n",
    "    x = get_x(row, D)\n",
    "\n",
    "    # step 2, get prediction\n",
    "    p = get_p(x, w)\n",
    "\n",
    "    # for progress validation, useless for learning our model\n",
    "    loss += logloss(p, y)\n",
    "    if t % 1000000 == 0 and t > 1:\n",
    "        print('%s\\tencountered: %d\\tcurrent logloss: %f' % (\n",
    "            datetime.now(), t, loss/t))\n",
    "\n",
    "    # step 3, update model with answer\n",
    "    w, n = update_w(w, n, x, p, y)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# testing (build kaggle's submission file)\n",
    "c_sum = 0\n",
    "with open('submission1234.csv', 'w') as submission:\n",
    "    submission.write('Id,Click\\n')\n",
    "    for t, row in enumerate(DictReader(open(test))):\n",
    "        Id = t + 1\n",
    "        del row['Label']\n",
    "        del row['timestamp']\n",
    "        x = get_x(row, D)\n",
    "        p = get_p(x, w)\n",
    "        if p > 0.5:\n",
    "            c_sum += 1\n",
    "        submission.write('%s,%f\\n' % (Id, p))\n",
    "        if t % 1000000 == 0 and t > 1:\n",
    "            print('%s\\tencountered: %d\\n' % (datetime.now(), t))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "c_sum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for t, row in enumerate(DictReader(open(train))):\n",
    "    print(t)\n",
    "    print(row)\n",
    "    if t == 1:\n",
    "        break\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "from csv import DictReader\n",
    "\n",
    "def csv_to_vw(loc_csv, loc_output, train=True):\n",
    "\n",
    "    c_sum = 0\n",
    "    start = datetime.now()\n",
    "    print(\"\\nTurning %s into %s. Is_train_set? %s\"%(loc_csv,loc_output,train))\n",
    "\n",
    "    with open(loc_output,\"w\") as outfile:\n",
    "        for e, row in enumerate( DictReader(open(loc_csv)) ):\n",
    "            \n",
    "            #Creating the features\n",
    "            numerical_features = \"\"\n",
    "            categorical_features = \"\"\n",
    "            for k,v in row.items():\n",
    "                if k not in [\"Label\",\"timestamp\"]:\n",
    "                    if \"I\" in k: # numerical feature, example: I5\n",
    "                        if len(str(v)) > 0: #check for empty values\n",
    "                            numerical_features += \" %s:%s\" % (k,v)\n",
    "                    if \"C\" in k: # categorical feature, example: C2\n",
    "                        if len(str(v)) > 0:\n",
    "                            categorical_features += \" %s\" % v\n",
    "            \n",
    "            #Creating the labels\n",
    "            if train: \n",
    "                if row['Label'] == \"1\":\n",
    "                    label = 1\n",
    "                    c_sum += 1\n",
    "                else:\n",
    "                    label = -1 #we set negative label to -1\n",
    "                outfile.write( \"%s '%s |i%s |c%s\\n\" % (label,row['timestamp'],numerical_features,categorical_features) )\n",
    "            \n",
    "            else: #we dont care about labels\n",
    "                outfile.write( \"1 '%s |i%s |c%s\\n\" % (row['timestamp'],numerical_features,categorical_features) )\n",
    "            \n",
    "            #Reporting progress\n",
    "            if e % 1000000 == 0:\n",
    "                 print(\"%s\\t%s\"%(e, str(datetime.now() - start)))\n",
    "            \n",
    "    print(\"\\n %s Task execution time:\\n\\t%s\"%(e, str(datetime.now() - start)))\n",
    "    print(c_sum)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "csv_to_vw(\"data/train_2.csv\", \"data/train_2.vw\",train=True)\n",
    "csv_to_vw(\"data/test_2.csv\", \"data/test_2.vw\",train=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "def zygmoid(x):\n",
    "\treturn 1 / (1 + math.exp(-x))\n",
    "\n",
    "with open(\"kaggle.click.submission.csv\",\"w\") as outfile:\n",
    "    outfile.write(\"Id,Click\\n\")\n",
    "    i = 1\n",
    "    for line in open(\"data/click.preds.txt\"):\n",
    "             row = line.strip().split(\" \")\n",
    "             outfile.write(\"%s,%f\\n\"%(i,zygmoid(float(row[0]))))\n",
    "             i = i + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sub_r = pd.read_csv (\"submission1234.csv\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sub_r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted(sub_r['Click'], reverse=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted(sub_r['Click'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sub_r['Click'].values[sub_r['Click'] < 0.01] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sub_r.to_csv(\"submission12345.csv\", index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
