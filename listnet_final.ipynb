{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys,os\n",
    "\n",
    "import numpy as np\n",
    "import six\n",
    "import pickle\n",
    "import scipy\n",
    "import chainer\n",
    "import chainer.functions as F\n",
    "import chainer.links as L\n",
    "from chainer import optimizers\n",
    "from chainer import serializers\n",
    "from tqdm import tqdm\n",
    "import scipy.stats as ss\n",
    "from sklearn.preprocessing import StandardScaler\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import combinations\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "class Net(nn.Module):\n",
    "    def __init__(self, D):\n",
    "        super(Net, self).__init__()\n",
    "        self.l1 = nn.Linear(D, 10)\n",
    "        self.l2 = nn.Linear(10, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = torch.sigmoid(self.l1(x))\n",
    "        x = self.l2(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "def listnet_loss(y_i, z_i):\n",
    "    \"\"\"\n",
    "    y_i: (n_i, 1)\n",
    "    z_i: (n_i, 1)\n",
    "    \"\"\"\n",
    "\n",
    "    P_y_i = F.softmax(y_i, dim=0)\n",
    "    P_z_i = F.softmax(z_i, dim=0)\n",
    "    return - torch.sum(P_y_i * torch.log(P_z_i))\n",
    "\n",
    "\n",
    "def make_dataset(N_train, N_valid, D):\n",
    "    ws = torch.randn(D, 1)\n",
    "\n",
    "    X_train = torch.randn(N_train, D, requires_grad=True)\n",
    "    X_valid = torch.randn(N_valid, D, requires_grad=True)\n",
    "\n",
    "    ys_train_score = torch.mm(X_train, ws)\n",
    "    ys_valid_score = torch.mm(X_valid, ws)\n",
    "\n",
    "    bins = [-2, -1, 0, 1]  # 5 relevances\n",
    "    ys_train_rel = torch.Tensor(\n",
    "        np.digitize(ys_train_score.clone().detach().numpy(), bins=bins)\n",
    "    )\n",
    "    ys_valid_rel = torch.Tensor(\n",
    "        np.digitize(ys_valid_score.clone().detach().numpy(), bins=bins)\n",
    "    )\n",
    "\n",
    "    return X_train, X_valid, ys_train_rel, ys_valid_rel\n",
    "\n",
    "\n",
    "def swapped_pairs(ys_pred, ys_target):\n",
    "    N = ys_target.shape[0]\n",
    "    swapped = 0\n",
    "    for i in range(N - 1):\n",
    "        for j in range(i + 1, N):\n",
    "            if ys_target[i] < ys_target[j]:\n",
    "                if ys_pred[i] > ys_pred[j]:\n",
    "                    swapped += 1\n",
    "            elif ys_target[i] > ys_target[j]:\n",
    "                if ys_pred[i] < ys_pred[j]:\n",
    "                    swapped += 1\n",
    "    return swapped\n",
    "\n",
    "\n",
    "def ndcg(ys_true, ys_pred):\n",
    "    def dcg(ys_true, ys_pred):\n",
    "        _, argsort = torch.sort(ys_pred, descending=True, dim=0)\n",
    "        ys_true_sorted = ys_true[argsort]\n",
    "        ret = 0\n",
    "        for i, l in enumerate(ys_true_sorted, 1):\n",
    "            ret += (2 ** l - 1) / np.log2(1 + i)\n",
    "        return ret\n",
    "    ideal_dcg = dcg(ys_true, ys_true)\n",
    "    pred_dcg = dcg(ys_true, ys_pred)\n",
    "    return pred_dcg / ideal_dcg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_svmlight_file\n",
    "X, y, query_ids = load_svmlight_file('/Users/lmac/Documents/inf_search/learning_to_rank/l2r/train.txt', query_id=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = X.todense()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.1, random_state=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = torch.from_numpy(X_train).float()\n",
    "X_valid = torch.from_numpy(X_test).float()\n",
    "ys_train = torch.from_numpy(y_train).float()\n",
    "ys_valid = torch.from_numpy(y_test).float()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tqdm\n",
    "N_train = X_train.size()[0]\n",
    "N_valid = X_valid.size()[0]\n",
    "D = X_train.size()[1]\n",
    "epochs = 10\n",
    "batch_size = 10000\n",
    "\n",
    "net = Net(D)\n",
    "opt = optim.Adam(net.parameters())\n",
    "\n",
    "for epoch in range(epochs):\n",
    "        idx = torch.randperm(N_train)\n",
    "\n",
    "        X_train = X_train[idx]\n",
    "        ys_train = ys_train[idx]\n",
    "\n",
    "        cur_batch = 0\n",
    "        for it in tqdm.tqdm(range(N_train // batch_size)):\n",
    "            batch_X = X_train[cur_batch: cur_batch + batch_size]\n",
    "            batch_ys = ys_train[cur_batch: cur_batch + batch_size]\n",
    "            cur_batch += batch_size\n",
    "\n",
    "            opt.zero_grad()\n",
    "            if len(batch_X) > 0:\n",
    "                batch_pred = net(batch_X)\n",
    "                batch_loss = listnet_loss(batch_ys, batch_pred)\n",
    "                batch_loss.backward(retain_graph=True)\n",
    "                opt.step()\n",
    "\n",
    "        with torch.no_grad():\n",
    "            valid_pred = net(X_valid)\n",
    "            valid_swapped_pairs = swapped_pairs(valid_pred, ys_valid)\n",
    "            ndcg_score = ndcg(ys_valid, valid_pred).item()\n",
    "            print(f\"epoch: {epoch + 1} valid swapped pairs: {valid_swapped_pairs}/{N_valid * (N_valid - 1) // 2} ndcg: {ndcg_score:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np \n",
    "import pandas as pd\n",
    "import ast\n",
    "import time\n",
    "from sklearn import preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm_notebook as tqdm\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "def get_list_xy(data,query_id='qid',relevancy='relevance degree'):\n",
    "\n",
    "    \"\"\"\n",
    "    returns a dataframe, sectioned by queries in order to train based on them\n",
    "    \"\"\"\n",
    "    all_queries=data[query_id].values\n",
    "    indexes = np.unique(all_queries, return_index=True)[1]\n",
    "    queries=[all_queries[index] for index in sorted(indexes)]\n",
    "\n",
    "    y_list=[]\n",
    "    x_list=[]\n",
    "    for i in range(len(queries)):\n",
    "        data_q=data[data[query_id]==queries[i]]\n",
    "        q_x=data_q.drop([query_id,relevancy],axis=1)\n",
    "        q_y=data_q[relevancy]\n",
    "        y_list.append(q_y)\n",
    "        x_list.append(q_x)\n",
    "\n",
    "    return x_list,y_list\n",
    "\n",
    "def create_folder(directory):\n",
    "\n",
    "    if not os.path.exists(directory):\n",
    "        os.makedirs(directory)\n",
    "        print(\"The directory\",directory,\" did not exist, and was created\")\n",
    "    else:\n",
    "        print(\"The directory\",directory,\" already exists\")\n",
    "        \n",
    "def create_folders(path,new_folders):\n",
    "    \"\"\"\n",
    "    Checks if a set of folders exist, and if not, creates them\n",
    "    \"\"\"\n",
    "    for folder in new_folders:\n",
    "        create_folder(path+folder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import load_model\n",
    "from keras.initializers import glorot_uniform\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.optimizers import SGD\n",
    "from keras.layers import Dropout\n",
    "import tensorflow.keras.backend as K\n",
    "import random\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import keras\n",
    "from keras.initializers import Constant\n",
    "from tqdm import tqdm_notebook as tqdm\n",
    "\n",
    "import time\n",
    "\n",
    "\n",
    "def Pz_keras(y_pred):\n",
    "    '''\n",
    "    Top1 probability, described in the original ListNet paper\n",
    "    '''\n",
    "\n",
    "    return K.exp(y_pred)/K.sum(K.exp(y_pred))\n",
    "\n",
    "\n",
    "def Loss_query_keras(y,y_pred):\n",
    "    return -K.sum(Pz_keras(y)*K.log(Pz_keras(y_pred)))\n",
    "\n",
    "def create_model(number_of_features,\n",
    "                 optimizer,\n",
    "                 initializer=tf.keras.initializers.glorot_uniform(seed=1),\n",
    "                 neurons_per_layer=[1],\n",
    "                 activation_function='relu',\n",
    "                 final_activation='sigmoid',\n",
    "                 dropout=0,\n",
    "                 bias=0.1):\n",
    "    modelq = Sequential()\n",
    "    modelq.add(Dense(neurons_per_layer[0], \n",
    "                     input_dim=number_of_features,\n",
    "                     activation=activation_function,\n",
    "                     kernel_initializer=initializer,\n",
    "                     bias_initializer=Constant(value=bias)))\n",
    "    if(neurons_per_layer[0]>1):\n",
    "        modelq.add(Dropout(dropout))\n",
    "    if(len(neurons_per_layer)>1 or neurons_per_layer[-1]>1):\n",
    "        for neurons in neurons_per_layer[1:]:\n",
    "            modelq.add(Dense(neurons, activation=activation_function,\n",
    "                             kernel_initializer=initializer,\n",
    "                             bias_initializer=Constant(value=bias)))\n",
    "            modelq.add(Dropout(dropout))\n",
    "        \n",
    "        modelq.add(Dense(1, activation=final_activation,\n",
    "                         kernel_initializer=initializer,\n",
    "                         bias_initializer=Constant(value=bias)))\n",
    "    \n",
    "    modelq.compile(loss=Loss_query_keras, optimizer=optimizer)\n",
    "    return modelq\n",
    "\n",
    "def generate_predictions_grid(path,\n",
    "                              data_vali,\n",
    "                              data_train,\n",
    "                              data_test,\n",
    "                              x_list_train,\n",
    "                              y_list_train,\n",
    "                              epochs=[50],\n",
    "                              learning_rate=[0.075],\n",
    "                              mon=[0.5],\n",
    "                              act=['relu'],\n",
    "                              number_neurons=[1],\n",
    "                              dropout=[0.5],\n",
    "                              final_activ=['linear'],\n",
    "                              hidden_layers=[0],\n",
    "                              name='',\n",
    "                              index=0.5,\n",
    "                              decresing_architecture=False,\n",
    "                              show_summary=False,\n",
    "                              save_model=False):\n",
    "\n",
    "    t_ini=time.time()\n",
    "    total_it=len(epochs)*len(learning_rate)*len(mon)*\\\n",
    "    len(act)*len(number_neurons)*len(dropout)*\\\n",
    "    len(final_activ)*len(hidden_layers)\n",
    "\n",
    "    print(\"\\n\\n\\nName of predictions will be of form: \"+\\\n",
    "          name+\"<index>,\\nwith <index> starting at\",index,\n",
    "          \"finishing at\",index+total_it-1,\"\\n\\n\\n_\")\n",
    "    name_orig=name\n",
    "\n",
    "    counter=1\n",
    "    for my_iter in (product(epochs,learning_rate,mon,act,number_neurons,\n",
    "                                dropout,final_activ,hidden_layers)):\n",
    "        name=name_orig+str(index)\n",
    "        n_iter=my_iter[0]\n",
    "        n_=my_iter[1]\n",
    "        mom_=my_iter[2]\n",
    "        act_=my_iter[3]\n",
    "        n_neurons_=my_iter[4]\n",
    "        dropout_=my_iter[5]\n",
    "        final_act=my_iter[6]\n",
    "        hidden_layers_=my_iter[7]\n",
    "        opt = SGD(lr=n_, momentum=mom_)\n",
    "        if(hidden_layers_>0):\n",
    "\n",
    "\t        neurons_per_layer=np.repeat(n_neurons_,hidden_layers_)\n",
    "\t        if(decresing_architecture):\n",
    "\t            neurons_per_layer=np.array([neuron//2**index for index,neuron in \\\n",
    "\t                                        enumerate(neurons_per_layer)])\n",
    "\t            neurons_per_layer=neurons_per_layer[neurons_per_layer>1]\n",
    "        else:\n",
    "        \tneurons_per_layer=[n_neurons_]\n",
    "\n",
    "\n",
    "        modelq=create_model(optimizer=opt,\n",
    "                        number_of_features=x_list_train[0].shape[1],\n",
    "                        neurons_per_layer=neurons_per_layer,\n",
    "                        activation_function=act_,\n",
    "                        final_activation=final_act,\n",
    "                        dropout=dropout_)\n",
    "        if(show_summary):\n",
    "            display(modelq.summary())\n",
    "\n",
    "\n",
    "        for j in tqdm(range(n_iter)):\n",
    "\n",
    "            for ki in range(len(y_list_train)):\n",
    "                if final_act=='sigmoid':\n",
    "                    loss_qi=modelq.train_on_batch(x=x_list_train[ki],y=y_list_train[ki]/2)\n",
    "                else:\n",
    "                    loss_qi=modelq.train_on_batch(x=x_list_train[ki],y=y_list_train[ki])\n",
    "\n",
    "        print(\"Iteration:\",counter,\"/\",total_it)\n",
    "        print(\"Time elapsed so far:\")\n",
    "        print(convert_to_time(time.time()-t_ini))\n",
    "        print(my_iter,\"\\n\\n\\n\")\n",
    "        if(save_model):\n",
    "            modelq.save(path+'models/model_'+name+'.h5')\n",
    "        index+=1\n",
    "        counter+=1\n",
    "\n",
    "        y_pred_train=modelq.predict(data_train.drop(['relevance degree','qid'],axis=1)).astype('float64').ravel()\n",
    "        df_train=pd.DataFrame(y_pred_train)\n",
    "        df_train.to_csv(path+'predictions/y_train_'+name+'.txt',sep=' ',header=False,index=False)\n",
    "\n",
    "        y_pred_vali=modelq.predict(data_vali.drop(['relevance degree','qid'],axis=1)).astype('float64').ravel()\n",
    "        df_vali=pd.DataFrame(y_pred_vali)\n",
    "        df_vali.to_csv(path+'predictions/y_vali_'+name+'.txt',sep=' ',header=False,index=False)\n",
    "\n",
    "        y_pred_test=modelq.predict(data_test.drop(['relevance degree','qid'],axis=1)).astype('float64').ravel()\n",
    "        df_test=pd.DataFrame(y_pred_test)\n",
    "        df_test.to_csv(path+'predictions/y_test_'+name+'.txt',sep=' ',header=False,index=False)\n",
    "\n",
    "\n",
    "def convert_to_time(seconds): \n",
    "    return time.strftime(\"(Hours:Minutes:Seconds)\\n%H:%M:%S\", time.gmtime(seconds)) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense,Dropout\n",
    "from keras.optimizers import SGD\n",
    "import time\n",
    "from keras.regularizers import l1\n",
    "t_ini=time.time()\n",
    "from itertools import product,count\n",
    "epochs = [100]\n",
    "learning_rate=[0.001]\n",
    "mon=[0.5]\n",
    "act=['linear']\n",
    "initializer=keras.initializers.glorot_uniform(seed=1)\n",
    "bias=0.1\n",
    "total_it=len(epochs)*len(learning_rate)*len(mon)*len(act)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_model=False "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def linear_model_iteration(my_iter,name,counter):\n",
    "\n",
    "    print(\"Iteration:\",counter,\"/\",total_it)\n",
    "    name=name+str(counter)\n",
    "    \n",
    "    x_list_train,y_list_train=get_list_xy(data_train)\n",
    "    \n",
    "    n_iter=my_iter[0]\n",
    "    n_=my_iter[1]\n",
    "    mom_=my_iter[2]\n",
    "    act_=my_iter[3]\n",
    "\n",
    "    modelq = Sequential()\n",
    "    modelq.add(Dense(1,input_dim=x_list_train[0].shape[1], activation=act_,\n",
    "                     kernel_initializer=initializer, bias_initializer=Constant(value=bias)))\n",
    "    opt = SGD(lr=n_, momentum=mom_)\n",
    "\n",
    "    modelq.compile(loss=Loss_query_keras, optimizer=opt)\n",
    "    \n",
    "    for j in tqdm(range(n_iter)):\n",
    "\n",
    "        for ki in range(len(y_list_train)):\n",
    "            if act_=='sigmoid':\n",
    "                loss_qi=modelq.train_on_batch(x=x_list_train[ki],y=y_list_train[ki]/2)\n",
    "            else:\n",
    "                loss_qi=modelq.train_on_batch(x=x_list_train[ki],y=y_list_train[ki])\n",
    "            \n",
    "            \n",
    "\n",
    "    \n",
    "    print(\"Time elapsed so far:\")\n",
    "    print(convert_to_time(time.time()-t_ini))\n",
    "    print(my_iter,\"\\n\\n\\n\")\n",
    "    if(save_model):\n",
    "        modelq.save(path+'new_models/model_'+name+'.h5')\n",
    "    \n",
    "\n",
    "    y_pred_train=modelq.predict(data_train.drop(['relevance degree','qid'],axis=1)).astype('float64').ravel()\n",
    "    df_train=pd.DataFrame(y_pred_train)\n",
    "    df_train.to_csv(path+'new_predictions/y_train_'+name+'.txt',sep=' ',header=False,index=False)\n",
    "\n",
    "    y_pred_vali=modelq.predict(data_vali.drop(['relevance degree','qid'],axis=1)).astype('float64').ravel()\n",
    "    df_vali=pd.DataFrame(y_pred_vali)\n",
    "    df_vali.to_csv(path+'new_predictions/y_vali_'+name+'.txt',sep=' ',header=False,index=False)\n",
    "\n",
    "\n",
    "    y_pred_test=modelq.predict(data_test.drop(['relevance degree','qid'],axis=1)).astype('float64').ravel()\n",
    "    df_test=pd.DataFrame(y_pred_test)\n",
    "    df_test.to_csv(path+'new_predictions/y_test_'+name+'.txt',sep=' ',header=False,index=False)\n",
    "    print(\"                             \",counter,\"                                     \")\n",
    "    print(\"=================================================================\\n\\n\\n\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_svmlight_file\n",
    "X, y, query_ids = load_svmlight_file('/Users/lmac/Documents/inf_search/learning_to_rank/l2r/train.txt', query_id=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def form_df(X, y):\n",
    "\n",
    "    data = pd.DataFrame(X)\n",
    "\n",
    "    features_names = []\n",
    "    for i in range(X.shape[1]):\n",
    "          features_names.append('f'+str(i))\n",
    "    data.columns = features_names\n",
    "    data=data.rename(index=str, columns={\"f0\": \"qid\"})\n",
    "    data['relevance degree'] = y\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query_ids = np.array(query_ids)\n",
    "X = X.todense()\n",
    "X = np.array(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tmp_X = np.column_stack((query_ids, X))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(tmp_X, y, test_size=0.1, random_state=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_vali=form_df(X_test, y_test)\n",
    "data_train=form_df(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_svmlight_file\n",
    "testX, testy, testquery_ids = load_svmlight_file('/Users/lmac/Documents/inf_search/learning_to_rank/l2r/test.txt', query_id=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "testquery_ids = np.array(testquery_ids)\n",
    "testX = testX.todense()\n",
    "testX = np.array(testX)\n",
    "\n",
    "testX = np.column_stack((testquery_ids, testX))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_test=form_df(testX, testy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = '/Users/lmac/Documents/inf_search/learning_to_rank'\n",
    "\n",
    "create_folders(path,[\"new_predictions\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = [100]\n",
    "for(my_iter,counter) in zip(product(epochs,learning_rate,mon,act), range(1,total_it+1)):\n",
    "    linear_model_iteration(my_iter, \"test\", counter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = np.loadtxt('/Users/lmac/Documents/inf_search/learning_to_ranknew_predictions/y_test_test1.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query_ids_test = testquery_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "test_query_groups = defaultdict(list)\n",
    "for doc_id, query_id in enumerate(query_ids_test):\n",
    "    test_query_groups[query_id].append(doc_id)\n",
    "with open(\"my_subm_test.csv\", 'w') as outp:\n",
    "    print(\"QueryId,DocumentId\", file=outp)\n",
    "    for query_id in test_query_groups:\n",
    "        docs_ids = test_query_groups[query_id]\n",
    "        y_pred_for_query = y_pred[docs_ids]\n",
    "        sorted_ids = np.argsort(y_pred_for_query)[::-1]\n",
    "        ranked_docs = np.array(docs_ids)[sorted_ids]\n",
    "        for doc_id in ranked_docs:\n",
    "            print(f\"{query_id},{doc_id+1}\", file=outp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense,Dropout\n",
    "from keras.optimizers import SGD\n",
    "import time\n",
    "from keras.regularizers import l1\n",
    "t_ini=time.time()\n",
    "from itertools import product\n",
    "learning_rate=[0.0025]\n",
    "mon=[0.5]\n",
    "act=['linear']\n",
    "inner_act=['relu']\n",
    "number_layers=[2]\n",
    "number_neurons=[100]\n",
    "dropout=[0.65]\n",
    "counter=1\n",
    "initializer=keras.initializers.glorot_uniform(seed=my_seed)\n",
    "bias=0.1\n",
    "epochs = [100]\n",
    "total_it=len(epochs)*len(learning_rate)*len(mon)*len(act)*len(inner_act)\\\n",
    "*len(number_layers)*len(number_neurons)*len(dropout)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def NN_model_iteration(my_iter,name,counter):\n",
    "\n",
    "    name=name+str(counter)\n",
    "    print(\"Iteration:\",counter,\"/\",total_it)\n",
    "\n",
    "    x_list_train,y_list_train=get_list_xy(data_train)\n",
    "    \n",
    "    n_iter=my_iter[0]\n",
    "    n_=my_iter[1]\n",
    "    mom_=my_iter[2]\n",
    "    act_=my_iter[3]\n",
    "    \n",
    "    inner_act_=my_iter[4]\n",
    "    number_layers_=my_iter[5]\n",
    "    number_neurons_=my_iter[6]\n",
    "    dropout_=my_iter[7]\n",
    "\n",
    "    modelq = Sequential()\n",
    "    modelq.add(Dense(number_neurons_,input_dim=x_list_train[0].shape[1], activation=inner_act_,\n",
    "                     kernel_initializer=initializer, bias_initializer=Constant(value=bias)))\n",
    "    modelq.add(Dropout(dropout_))\n",
    "    # Adding hidden layers based on the gridsearch value\n",
    "    \n",
    "    for i in range(number_layers_):\n",
    "        modelq.add(Dense(number_neurons_,activation=inner_act_,\n",
    "                        kernel_initializer=initializer,\n",
    "                        bias_initializer=Constant(value=bias)))\n",
    "        modelq.add(Dropout(dropout_))\n",
    "\n",
    "    modelq.add(Dense(1,activation=act_,\n",
    "                kernel_initializer=initializer,\n",
    "                bias_initializer=Constant(value=bias)))\n",
    "    opt = SGD(lr=n_, momentum=mom_)\n",
    "\n",
    "    modelq.compile(loss=Loss_query_keras, optimizer=opt)\n",
    "    #display(modelq.summary())\n",
    "    \n",
    "\n",
    "    for j in tqdm(range(n_iter)):\n",
    "\n",
    "        for ki in range(len(y_list_train)):\n",
    "            if act_=='sigmoid':\n",
    "                loss_qi=modelq.train_on_batch(x=x_list_train[ki],y=y_list_train[ki]/2)\n",
    "            else:\n",
    "                loss_qi=modelq.train_on_batch(x=x_list_train[ki],y=y_list_train[ki])\n",
    "            \n",
    "            \n",
    "\n",
    "    \n",
    "    print(\"Time elapsed so far:\")\n",
    "    print(convert_to_time(time.time()-t_ini))\n",
    "    print(my_iter,\"\\n\\n\\n\")\n",
    "    if(save_model):\n",
    "        modelq.save(path+'models/model_'+name+'.h5')\n",
    "    \n",
    "\n",
    "    y_pred_train=modelq.predict(data_train.drop(['relevance degree','qid'],axis=1)).astype('float64').ravel()\n",
    "    df_train=pd.DataFrame(y_pred_train)\n",
    "    df_train.to_csv(path+'new_predictions/y_train_'+name+'.txt',sep=' ',header=False,index=False)\n",
    "\n",
    "    y_pred_vali=modelq.predict(data_vali.drop(['relevance degree','qid'],axis=1)).astype('float64').ravel()\n",
    "    df_vali=pd.DataFrame(y_pred_vali)\n",
    "    df_vali.to_csv(path+'new_predictions/y_vali_'+name+'.txt',sep=' ',header=False,index=False)\n",
    "\n",
    "\n",
    "    y_pred_test=modelq.predict(data_test.drop(['relevance degree','qid'],axis=1)).astype('float64').ravel()\n",
    "    df_test=pd.DataFrame(y_pred_test)\n",
    "    df_test.to_csv(path+'new_predictions/y_test_'+name+'.txt',sep=' ',header=False,index=False)\n",
    "    print(\"                             \",counter,\"                                     \")\n",
    "    print(\"=================================================================\\n\\n\\n\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = [10]\n",
    "for(my_iter,counter) in zip(product(epochs,learning_rate,mon,act,\n",
    "                                                                   inner_act,number_layers,number_neurons,dropout),\n",
    "                                                           range(1,total_it+1)):\n",
    "    NN_model_iteration(my_iter, \"test2\", counter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = [100]\n",
    "for(my_iter,counter) in zip(product(epochs,learning_rate,mon,act,\n",
    "                                                                   inner_act,number_layers,number_neurons,dropout),\n",
    "                                                           range(1,total_it+1)):\n",
    "    NN_model_iteration(my_iter, \"test3\", counter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = np.loadtxt('/Users/lmac/Documents/inf_search/learning_to_ranknew_predictions/y_test_test21.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "test_query_groups = defaultdict(list)\n",
    "for doc_id, query_id in enumerate(query_ids_test):\n",
    "    test_query_groups[query_id].append(doc_id)\n",
    "with open(\"my_subm_test3.csv\", 'w') as outp:\n",
    "    print(\"QueryId,DocumentId\", file=outp)\n",
    "    for query_id in test_query_groups:\n",
    "        docs_ids = test_query_groups[query_id]\n",
    "        y_pred_for_query = y_pred[docs_ids]\n",
    "        sorted_ids = np.argsort(y_pred_for_query)[::-1]\n",
    "        ranked_docs = np.array(docs_ids)[sorted_ids]\n",
    "        for doc_id in ranked_docs:\n",
    "            print(f\"{query_id},{doc_id+1}\", file=outp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
